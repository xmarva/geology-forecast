{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baab6801",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:48.681960Z",
     "iopub.status.busy": "2025-03-30T02:32:48.681677Z",
     "iopub.status.idle": "2025-03-30T02:32:56.836035Z",
     "shell.execute_reply": "2025-03-30T02:32:56.835328Z"
    },
    "papermill": {
     "duration": 8.161863,
     "end_time": "2025-03-30T02:32:56.837669",
     "exception": false,
     "start_time": "2025-03-30T02:32:48.675806",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import logsumexp\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import wandb\n",
    "from google.colab import userdata\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3745aa20",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:56.846970Z",
     "iopub.status.busy": "2025-03-30T02:32:56.846738Z",
     "iopub.status.idle": "2025-03-30T02:32:56.850693Z",
     "shell.execute_reply": "2025-03-30T02:32:56.850099Z"
    },
    "papermill": {
     "duration": 0.009801,
     "end_time": "2025-03-30T02:32:56.851876",
     "exception": false,
     "start_time": "2025-03-30T02:32:56.842075",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "# seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ee1ea069",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:56.860259Z",
     "iopub.status.busy": "2025-03-30T02:32:56.860030Z",
     "iopub.status.idle": "2025-03-30T02:32:56.868231Z",
     "shell.execute_reply": "2025-03-30T02:32:56.867645Z"
    },
    "papermill": {
     "duration": 0.013665,
     "end_time": "2025-03-30T02:32:56.869364",
     "exception": false,
     "start_time": "2025-03-30T02:32:56.855699",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from kaggle_secrets import UserSecretsClient\n",
    "\n",
    "def init_wandb(project_name=\"geology-forecast-challenge\", config=None):\n",
    "    try:\n",
    "        user_secrets = UserSecretsClient()\n",
    "        \n",
    "        wandb_api_key = user_secrets.get_secret(\"wandb\")\n",
    "        os.environ['WANDB_API_KEY'] = wandb_api_key\n",
    "        \n",
    "        wandb.login(key=wandb_api_key)\n",
    "        \n",
    "        run = wandb.init(\n",
    "            project=project_name,\n",
    "            config=config,\n",
    "            tags=[\"LSTM\", \"Geology Forecast Challenge\"],\n",
    "        )\n",
    "        \n",
    "        print(\"W&B successfully initialized\")\n",
    "        return run\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing W&B: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fc3eca2d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:56.877546Z",
     "iopub.status.busy": "2025-03-30T02:32:56.877347Z",
     "iopub.status.idle": "2025-03-30T02:32:56.933424Z",
     "shell.execute_reply": "2025-03-30T02:32:56.932639Z"
    },
    "papermill": {
     "duration": 0.061453,
     "end_time": "2025-03-30T02:32:56.934656",
     "exception": false,
     "start_time": "2025-03-30T02:32:56.873203",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "47c5007c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:56.943485Z",
     "iopub.status.busy": "2025-03-30T02:32:56.943244Z",
     "iopub.status.idle": "2025-03-30T02:32:59.353099Z",
     "shell.execute_reply": "2025-03-30T02:32:59.352135Z"
    },
    "papermill": {
     "duration": 2.416057,
     "end_time": "2025-03-30T02:32:59.354734",
     "exception": false,
     "start_time": "2025-03-30T02:32:56.938677",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/kaggle/input/geology-forecast-challenge-open/data/train.csv\").fillna(0)\n",
    "test = pd.read_csv(\"/kaggle/input/geology-forecast-challenge-open/data/test.csv\").fillna(0)\n",
    "sub = pd.read_csv('/kaggle/input/geology-forecast-challenge-open/data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3ce9b5e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.363787Z",
     "iopub.status.busy": "2025-03-30T02:32:59.363502Z",
     "iopub.status.idle": "2025-03-30T02:32:59.430724Z",
     "shell.execute_reply": "2025-03-30T02:32:59.429725Z"
    },
    "papermill": {
     "duration": 0.073384,
     "end_time": "2025-03-30T02:32:59.432353",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.358969",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "FEATURES = [c for c in test.columns if c != 'geology_id']\n",
    "TARGETS = [c for c in sub.columns if c != 'geology_id']\n",
    "solution = train[['geology_id'] + TARGETS].copy()\n",
    "train_sub = train[['geology_id'] + TARGETS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7618a640",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.441449Z",
     "iopub.status.busy": "2025-03-30T02:32:59.441186Z",
     "iopub.status.idle": "2025-03-30T02:32:59.446599Z",
     "shell.execute_reply": "2025-03-30T02:32:59.445809Z"
    },
    "papermill": {
     "duration": 0.011099,
     "end_time": "2025-03-30T02:32:59.447758",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.436659",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class LSTMForecastModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        output_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        \n",
    "        x = self.fc1(lstm_out)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "723f8c61",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.456171Z",
     "iopub.status.busy": "2025-03-30T02:32:59.455917Z",
     "iopub.status.idle": "2025-03-30T02:32:59.460323Z",
     "shell.execute_reply": "2025-03-30T02:32:59.459520Z"
    },
    "papermill": {
     "duration": 0.009914,
     "end_time": "2025-03-30T02:32:59.461511",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.451597",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class GeologyDataset(Dataset):\n",
    "    def __init__(self, features, targets=None, is_test=False):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.is_test = is_test\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        \n",
    "        x = x.reshape(-1, 1)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return x\n",
    "        else:\n",
    "            y = self.targets[idx]\n",
    "            return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b49c37f2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.469728Z",
     "iopub.status.busy": "2025-03-30T02:32:59.469523Z",
     "iopub.status.idle": "2025-03-30T02:32:59.472772Z",
     "shell.execute_reply": "2025-03-30T02:32:59.472168Z"
    },
    "papermill": {
     "duration": 0.008502,
     "end_time": "2025-03-30T02:32:59.473862",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.465360",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def preprocess_data(df, feature_cols, target_cols=None, is_test=False):\n",
    "    X = df[feature_cols].values\n",
    "    \n",
    "    if not is_test:\n",
    "        y = df[target_cols].values\n",
    "        return X, y\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "24f32e7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.482249Z",
     "iopub.status.busy": "2025-03-30T02:32:59.482004Z",
     "iopub.status.idle": "2025-03-30T02:32:59.488600Z",
     "shell.execute_reply": "2025-03-30T02:32:59.488001Z"
    },
    "papermill": {
     "duration": 0.012126,
     "end_time": "2025-03-30T02:32:59.489868",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.477742",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def compute_nll_score(solution, submission, row_id_column_name='geology_id'):\n",
    "    solution_copy = solution.copy()\n",
    "    submission_copy = submission.copy()\n",
    "    \n",
    "    del solution_copy[row_id_column_name]\n",
    "    del submission_copy[row_id_column_name]\n",
    "\n",
    "    NEGATIVE_PART = -299\n",
    "    LARGEST_CHUNK = 600\n",
    "    SMALLEST_CHUNK = 350\n",
    "    TOTAL_REALIZATIONS = 10\n",
    "    INFLATION_SIGMA = 600\n",
    "    \n",
    "    sigma_2 = np.ones((LARGEST_CHUNK+NEGATIVE_PART-1))\n",
    "    from_ranges = [1, 61, 245]\n",
    "    to_ranges_excl = [61, 245, 301]\n",
    "    log_slopes = [1.0406028049510443, 0.0, 7.835345062351012]\n",
    "    log_offsets = [-6.430669850650689, -2.1617411566043896, -45.24876794412965]\n",
    "\n",
    "    for growth_mode in range(len(from_ranges)):\n",
    "        for i in range(from_ranges[growth_mode], to_ranges_excl[growth_mode]):\n",
    "            sigma_2[i-1] = np.exp(np.log(i)*log_slopes[growth_mode]+log_offsets[growth_mode])\n",
    "\n",
    "    sigma_2 *= INFLATION_SIGMA\n",
    "  \n",
    "    cov_matrix_inv_diag = 1. / sigma_2\n",
    "    \n",
    "    num_rows = solution_copy.shape[0]\n",
    "    num_columns = LARGEST_CHUNK + NEGATIVE_PART - 1\n",
    "    \n",
    "    p = 1./TOTAL_REALIZATIONS\n",
    "    log_p = np.log(p)\n",
    "    \n",
    "    solution_arr = np.zeros((num_rows, TOTAL_REALIZATIONS, num_columns))\n",
    "    submission_arr = np.zeros((num_rows, TOTAL_REALIZATIONS, num_columns))\n",
    "    \n",
    "    for k in range(TOTAL_REALIZATIONS):\n",
    "        for i in range(num_columns):\n",
    "            if k == 0:\n",
    "                column_name = str(i+1)\n",
    "            else:\n",
    "                column_name = f\"r_{k}_pos_{i+1}\"\n",
    "            solution_arr[:, k, i] = solution_copy[column_name].values\n",
    "            submission_arr[:, k, i] = submission_copy[column_name].values\n",
    "\n",
    "    misfit = solution_arr - submission_arr\n",
    "    inner_product_matrix = np.sum(cov_matrix_inv_diag * misfit * misfit, axis=2)\n",
    "    \n",
    "    nll = -logsumexp(log_p - inner_product_matrix, axis=1)\n",
    "    \n",
    "    return nll.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "920ba5a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.498230Z",
     "iopub.status.busy": "2025-03-30T02:32:59.497983Z",
     "iopub.status.idle": "2025-03-30T02:32:59.502883Z",
     "shell.execute_reply": "2025-03-30T02:32:59.502112Z"
    },
    "papermill": {
     "duration": 0.010344,
     "end_time": "2025-03-30T02:32:59.504078",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.493734",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_model_with_nll_loss(model, train_loader, optimizer, device):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    for data, target in train_loader:\n",
    "        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_mean = target.mean(dim=0)\n",
    "        target_std = target.std(dim=0) + 1e-6\n",
    "        \n",
    "        normalized_output = (output - target_mean) / target_std\n",
    "        normalized_target = (target - target_mean) / target_std\n",
    "        \n",
    "        loss = F.mse_loss(normalized_output, normalized_target)\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        train_losses.append(loss.item())\n",
    "    \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d378dd22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.512362Z",
     "iopub.status.busy": "2025-03-30T02:32:59.512139Z",
     "iopub.status.idle": "2025-03-30T02:32:59.516945Z",
     "shell.execute_reply": "2025-03-30T02:32:59.516293Z"
    },
    "papermill": {
     "duration": 0.010356,
     "end_time": "2025-03-30T02:32:59.518225",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.507869",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in val_loader:\n",
    "            data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.float32)\n",
    "            output = model(data)\n",
    "            \n",
    "            target_mean = target.mean(dim=0)\n",
    "            target_std = target.std(dim=0) + 1e-6\n",
    "            \n",
    "            normalized_output = (output - target_mean) / target_std\n",
    "            normalized_target = (target - target_mean) / target_std\n",
    "            \n",
    "            loss = F.mse_loss(normalized_output, normalized_target)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            val_preds.append(output.cpu().numpy())\n",
    "            val_targets.append(target.cpu().numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_targets = np.concatenate(val_targets)\n",
    "    \n",
    "    return np.mean(val_losses), val_preds, val_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "99bc7b39",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.526676Z",
     "iopub.status.busy": "2025-03-30T02:32:59.526443Z",
     "iopub.status.idle": "2025-03-30T02:32:59.535991Z",
     "shell.execute_reply": "2025-03-30T02:32:59.535380Z"
    },
    "papermill": {
     "duration": 0.015129,
     "end_time": "2025-03-30T02:32:59.537231",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.522102",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train_and_predict(\n",
    "    fold_idx, \n",
    "    train_index, \n",
    "    val_index, \n",
    "    X_num, \n",
    "    y,\n",
    "    X_num_test,\n",
    "    config\n",
    "):\n",
    "    fold_config = config.copy()\n",
    "    fold_config.update({\"fold\": fold_idx})\n",
    "    \n",
    "    run = init_wandb(config=fold_config)\n",
    "    \n",
    "    X_num_train, X_num_val = X_num[train_index], X_num[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "    \n",
    "    train_dataset = GeologyDataset(X_num_train, y_train)\n",
    "    val_dataset = GeologyDataset(X_num_val, y_val)\n",
    "    test_dataset = GeologyDataset(X_num_test, is_test=True)\n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=True,\n",
    "        pin_memory=True, \n",
    "        num_workers=2  \n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        val_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size=config['batch_size'], \n",
    "        shuffle=False,\n",
    "        pin_memory=True,\n",
    "        num_workers=2\n",
    "    )\n",
    "    \n",
    "    model = LSTMForecastModel(\n",
    "        input_size=1,\n",
    "        hidden_size=config['hidden_size'],\n",
    "        num_layers=config['num_layers'],\n",
    "        output_size=len(TARGETS),\n",
    "        dropout=config['dropout']\n",
    "    ).to(device)\n",
    "    \n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=config['learning_rate'],\n",
    "        weight_decay=config['weight_decay'],\n",
    "        eps=1e-8  # Increased stability\n",
    "    )\n",
    "    \n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
    "        optimizer, \n",
    "        T_0=5,       \n",
    "        T_mult=2,   \n",
    "        eta_min=1e-6 \n",
    "    )\n",
    "    \n",
    "    best_val_loss = float('inf')\n",
    "    val_predictions = np.zeros((len(val_index), len(TARGETS)))\n",
    "    test_predictions = np.zeros((len(X_num_test), len(TARGETS)))\n",
    "    \n",
    "    print(f\"Training fold {fold_idx + 1}...\")\n",
    "    for epoch in range(config['epochs']):\n",
    "        train_loss = train_model_with_nll_loss(model, train_loader, optimizer, device)\n",
    "        \n",
    "        val_loss, val_preds, val_targets = validate_model(model, val_loader, device)\n",
    "        \n",
    "        val_predictions = val_preds\n",
    "        \n",
    "        scheduler.step()\n",
    "\n",
    "        if run:\n",
    "            run.log({\n",
    "                \"epoch\": epoch + 1,\n",
    "                \"train_loss\": train_loss,\n",
    "                \"val_loss\": val_loss,\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "            })\n",
    "        \n",
    "        print(f\"Epoch {epoch+1}/{config['epochs']} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "        \n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            model_path = f\"model_fold_{fold_idx}.pt\"\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            if run:\n",
    "                run.save(model_path)\n",
    "    \n",
    "    model.load_state_dict(torch.load(f\"model_fold_{fold_idx}.pt\"))\n",
    "    model.eval()\n",
    "    test_preds = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data in test_loader:\n",
    "            if isinstance(data, list):\n",
    "                data = data[0]  # For test data\n",
    "            data = data.to(device, dtype=torch.float32)\n",
    "            output = model(data)\n",
    "            test_preds.append(output.cpu().numpy())\n",
    "    \n",
    "    test_predictions = np.concatenate(test_preds)\n",
    "    \n",
    "    train_sub.loc[val_index, TARGETS] = val_predictions\n",
    "    \n",
    "    if run:\n",
    "        run.finish()\n",
    "    \n",
    "    return test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f4c4baa8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.545632Z",
     "iopub.status.busy": "2025-03-30T02:32:59.545342Z",
     "iopub.status.idle": "2025-03-30T02:32:59.548714Z",
     "shell.execute_reply": "2025-03-30T02:32:59.548105Z"
    },
    "papermill": {
     "duration": 0.009011,
     "end_time": "2025-03-30T02:32:59.549966",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.540955",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "config = {\n",
    "    'model_type': 'LSTM',\n",
    "    'hidden_size': 1024, \n",
    "    'num_layers': 3,\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 5e-4,\n",
    "    'weight_decay': 1e-5,\n",
    "    'batch_size': 128,\n",
    "    'epochs': 30,\n",
    "    'seed': SEED,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "09fa0d1f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.558236Z",
     "iopub.status.busy": "2025-03-30T02:32:59.557988Z",
     "iopub.status.idle": "2025-03-30T02:32:59.561193Z",
     "shell.execute_reply": "2025-03-30T02:32:59.560405Z"
    },
    "papermill": {
     "duration": 0.008768,
     "end_time": "2025-03-30T02:32:59.562503",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.553735",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "folds = 5\n",
    "kf = KFold(n_splits=folds, random_state=SEED, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "67694292",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.570902Z",
     "iopub.status.busy": "2025-03-30T02:32:59.570636Z",
     "iopub.status.idle": "2025-03-30T02:32:59.590388Z",
     "shell.execute_reply": "2025-03-30T02:32:59.589459Z"
    },
    "papermill": {
     "duration": 0.025563,
     "end_time": "2025-03-30T02:32:59.591886",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.566323",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_num, y = train[FEATURES].values, train[TARGETS].values\n",
    "X_num_test = test[FEATURES].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "35be52ff",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.600972Z",
     "iopub.status.busy": "2025-03-30T02:32:59.600747Z",
     "iopub.status.idle": "2025-03-30T02:32:59.604024Z",
     "shell.execute_reply": "2025-03-30T02:32:59.603347Z"
    },
    "papermill": {
     "duration": 0.009093,
     "end_time": "2025-03-30T02:32:59.605270",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.596177",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds_all_folds = np.zeros((folds, len(test), len(TARGETS)))\n",
    "val_scores = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7187495b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:32:59.613599Z",
     "iopub.status.busy": "2025-03-30T02:32:59.613394Z",
     "iopub.status.idle": "2025-03-30T02:53:18.895209Z",
     "shell.execute_reply": "2025-03-30T02:53:18.894194Z"
    },
    "papermill": {
     "duration": 1219.287608,
     "end_time": "2025-03-30T02:53:18.896645",
     "exception": false,
     "start_time": "2025-03-30T02:32:59.609037",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33meva-koroleva\u001b[0m (\u001b[33mml-samurai\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250330_023300-9bxqwuum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdeft-violet-10\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/9bxqwuum\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B successfully initialized\n",
      "Training fold 1...\n",
      "Epoch 1/30 - Train Loss: 1.711310, Val Loss: 1.032734\n",
      "Epoch 2/30 - Train Loss: 1.013712, Val Loss: 0.975252\n",
      "Epoch 3/30 - Train Loss: 0.841620, Val Loss: 0.601630\n",
      "Epoch 4/30 - Train Loss: 0.606527, Val Loss: 0.477440\n",
      "Epoch 5/30 - Train Loss: 0.520761, Val Loss: 0.431863\n",
      "Epoch 6/30 - Train Loss: 0.510234, Val Loss: 0.369546\n",
      "Epoch 7/30 - Train Loss: 0.393855, Val Loss: 0.287025\n",
      "Epoch 8/30 - Train Loss: 0.306541, Val Loss: 0.250319\n",
      "Epoch 9/30 - Train Loss: 0.272907, Val Loss: 0.206344\n",
      "Epoch 10/30 - Train Loss: 0.255032, Val Loss: 0.212533\n",
      "Epoch 11/30 - Train Loss: 0.248562, Val Loss: 0.230769\n",
      "Epoch 12/30 - Train Loss: 0.251923, Val Loss: 0.233824\n",
      "Epoch 13/30 - Train Loss: 0.248746, Val Loss: 0.221217\n",
      "Epoch 14/30 - Train Loss: 0.240724, Val Loss: 0.192614\n",
      "Epoch 15/30 - Train Loss: 0.230385, Val Loss: 0.191512\n",
      "Epoch 16/30 - Train Loss: 0.325586, Val Loss: 0.268739\n",
      "Epoch 17/30 - Train Loss: 0.279779, Val Loss: 0.206591\n",
      "Epoch 18/30 - Train Loss: 0.226744, Val Loss: 0.185215\n",
      "Epoch 19/30 - Train Loss: 0.224949, Val Loss: 0.193819\n",
      "Epoch 20/30 - Train Loss: 0.229854, Val Loss: 0.200036\n",
      "Epoch 21/30 - Train Loss: 0.208639, Val Loss: 0.193600\n",
      "Epoch 22/30 - Train Loss: 0.224248, Val Loss: 0.231894\n",
      "Epoch 23/30 - Train Loss: 0.213728, Val Loss: 0.173104\n",
      "Epoch 24/30 - Train Loss: 0.209615, Val Loss: 0.210313\n",
      "Epoch 25/30 - Train Loss: 0.234096, Val Loss: 0.206961\n",
      "Epoch 26/30 - Train Loss: 0.203825, Val Loss: 0.177015\n",
      "Epoch 27/30 - Train Loss: 0.200321, Val Loss: 0.174052\n",
      "Epoch 28/30 - Train Loss: 0.195540, Val Loss: 0.167980\n",
      "Epoch 29/30 - Train Loss: 0.198078, Val Loss: 0.176529\n",
      "Epoch 30/30 - Train Loss: 0.190781, Val Loss: 0.168011\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading config.yaml; uploading output.log; uploading history steps 28-29, summary, console lines 30-31; uploading model_fold_0.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading model_fold_0.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate â–‡â–†â–ƒâ–‚â–ˆâ–ˆâ–‡â–‡â–†â–„â–ƒâ–‚â–‚â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss â–ˆâ–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–â–â–‚â–‚â–â–â–â–‚â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 0.19078\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.16801\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mdeft-violet-10\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/9bxqwuum\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250330_023300-9bxqwuum/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1 validation NLL score: 21.342841\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250330_023707-oht661ia\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgenerous-yogurt-11\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/oht661ia\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B successfully initialized\n",
      "Training fold 2...\n",
      "Epoch 1/30 - Train Loss: 1.746221, Val Loss: 1.048943\n",
      "Epoch 2/30 - Train Loss: 1.022445, Val Loss: 1.018890\n",
      "Epoch 3/30 - Train Loss: 0.964448, Val Loss: 0.785485\n",
      "Epoch 4/30 - Train Loss: 0.731662, Val Loss: 0.559074\n",
      "Epoch 5/30 - Train Loss: 0.606865, Val Loss: 0.503792\n",
      "Epoch 6/30 - Train Loss: 0.545502, Val Loss: 0.388722\n",
      "Epoch 7/30 - Train Loss: 0.396994, Val Loss: 0.312898\n",
      "Epoch 8/30 - Train Loss: 0.339318, Val Loss: 0.293349\n",
      "Epoch 9/30 - Train Loss: 0.291813, Val Loss: 0.267485\n",
      "Epoch 10/30 - Train Loss: 0.258923, Val Loss: 0.248162\n",
      "Epoch 11/30 - Train Loss: 0.234493, Val Loss: 0.223121\n",
      "Epoch 12/30 - Train Loss: 0.219184, Val Loss: 0.231629\n",
      "Epoch 13/30 - Train Loss: 0.221750, Val Loss: 0.230914\n",
      "Epoch 14/30 - Train Loss: 0.218360, Val Loss: 0.234164\n",
      "Epoch 15/30 - Train Loss: 0.211238, Val Loss: 0.220548\n",
      "Epoch 16/30 - Train Loss: 0.699462, Val Loss: 0.292686\n",
      "Epoch 17/30 - Train Loss: 0.273170, Val Loss: 0.244605\n",
      "Epoch 18/30 - Train Loss: 0.251754, Val Loss: 0.216731\n",
      "Epoch 19/30 - Train Loss: 0.205140, Val Loss: 0.229177\n",
      "Epoch 20/30 - Train Loss: 0.196965, Val Loss: 0.228185\n",
      "Epoch 21/30 - Train Loss: 0.191892, Val Loss: 0.230710\n",
      "Epoch 22/30 - Train Loss: 0.201759, Val Loss: 0.247802\n",
      "Epoch 23/30 - Train Loss: 0.228809, Val Loss: 0.219793\n",
      "Epoch 24/30 - Train Loss: 0.190210, Val Loss: 0.243841\n",
      "Epoch 25/30 - Train Loss: 0.192838, Val Loss: 0.245908\n",
      "Epoch 26/30 - Train Loss: 0.200521, Val Loss: 0.230229\n",
      "Epoch 27/30 - Train Loss: 0.183837, Val Loss: 0.213811\n",
      "Epoch 28/30 - Train Loss: 0.180535, Val Loss: 0.231722\n",
      "Epoch 29/30 - Train Loss: 0.176164, Val Loss: 0.235505\n",
      "Epoch 30/30 - Train Loss: 0.176078, Val Loss: 0.219305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading wandb-summary.json; uploading output.log; uploading config.yaml; uploading model_fold_1.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading model_fold_1.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate â–‡â–†â–ƒâ–‚â–ˆâ–ˆâ–‡â–‡â–†â–„â–ƒâ–‚â–‚â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss â–ˆâ–…â–…â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–ƒâ–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss â–ˆâ–ˆâ–†â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 0.17608\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.21931\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mgenerous-yogurt-11\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/oht661ia\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250330_023707-oht661ia/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 2 validation NLL score: 32.929157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250330_024110-375opuch\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgraceful-silence-12\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/375opuch\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B successfully initialized\n",
      "Training fold 3...\n",
      "Epoch 1/30 - Train Loss: 1.776652, Val Loss: 1.016030\n",
      "Epoch 2/30 - Train Loss: 1.018708, Val Loss: 0.995725\n",
      "Epoch 3/30 - Train Loss: 0.899952, Val Loss: 0.669717\n",
      "Epoch 4/30 - Train Loss: 0.650563, Val Loss: 0.475193\n",
      "Epoch 5/30 - Train Loss: 0.557700, Val Loss: 0.444206\n",
      "Epoch 6/30 - Train Loss: 0.488322, Val Loss: 0.314413\n",
      "Epoch 7/30 - Train Loss: 0.412750, Val Loss: 0.263056\n",
      "Epoch 8/30 - Train Loss: 0.319082, Val Loss: 0.236475\n",
      "Epoch 9/30 - Train Loss: 0.290757, Val Loss: 0.218167\n",
      "Epoch 10/30 - Train Loss: 0.257597, Val Loss: 0.191627\n",
      "Epoch 11/30 - Train Loss: 0.239785, Val Loss: 0.206059\n",
      "Epoch 12/30 - Train Loss: 0.233780, Val Loss: 0.179116\n",
      "Epoch 13/30 - Train Loss: 0.227782, Val Loss: 0.181901\n",
      "Epoch 14/30 - Train Loss: 0.237396, Val Loss: 0.181718\n",
      "Epoch 15/30 - Train Loss: 0.215737, Val Loss: 0.180387\n",
      "Epoch 16/30 - Train Loss: 0.289397, Val Loss: 0.195331\n",
      "Epoch 17/30 - Train Loss: 0.253959, Val Loss: 0.246420\n",
      "Epoch 18/30 - Train Loss: 0.247300, Val Loss: 0.214698\n",
      "Epoch 19/30 - Train Loss: 0.240103, Val Loss: 0.178417\n",
      "Epoch 20/30 - Train Loss: 0.210432, Val Loss: 0.176434\n",
      "Epoch 21/30 - Train Loss: 0.208992, Val Loss: 0.185333\n",
      "Epoch 22/30 - Train Loss: 0.206537, Val Loss: 0.220391\n",
      "Epoch 23/30 - Train Loss: 0.230996, Val Loss: 0.188203\n",
      "Epoch 24/30 - Train Loss: 0.209245, Val Loss: 0.194998\n",
      "Epoch 25/30 - Train Loss: 0.206155, Val Loss: 0.180244\n",
      "Epoch 26/30 - Train Loss: 0.194847, Val Loss: 0.182594\n",
      "Epoch 27/30 - Train Loss: 0.188279, Val Loss: 0.186521\n",
      "Epoch 28/30 - Train Loss: 0.192477, Val Loss: 0.186590\n",
      "Epoch 29/30 - Train Loss: 0.189352, Val Loss: 0.169351\n",
      "Epoch 30/30 - Train Loss: 0.183738, Val Loss: 0.169423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading config.yaml; uploading output.log; uploading model_fold_2.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading model_fold_2.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate â–‡â–†â–ƒâ–‚â–ˆâ–ˆâ–‡â–‡â–†â–„â–ƒâ–‚â–‚â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss â–ˆâ–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 0.18374\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.16942\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mgraceful-silence-12\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/375opuch\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250330_024110-375opuch/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 3 validation NLL score: 24.249062\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250330_024513-bnl6rka4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mdecent-sponge-13\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/bnl6rka4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B successfully initialized\n",
      "Training fold 4...\n",
      "Epoch 1/30 - Train Loss: 1.679445, Val Loss: 1.001493\n",
      "Epoch 2/30 - Train Loss: 1.023592, Val Loss: 0.992910\n",
      "Epoch 3/30 - Train Loss: 0.926262, Val Loss: 0.689068\n",
      "Epoch 4/30 - Train Loss: 0.655811, Val Loss: 0.477894\n",
      "Epoch 5/30 - Train Loss: 0.544677, Val Loss: 0.426497\n",
      "Epoch 6/30 - Train Loss: 0.561778, Val Loss: 0.343427\n",
      "Epoch 7/30 - Train Loss: 0.433583, Val Loss: 0.285456\n",
      "Epoch 8/30 - Train Loss: 0.328617, Val Loss: 0.238977\n",
      "Epoch 9/30 - Train Loss: 0.292338, Val Loss: 0.267699\n",
      "Epoch 10/30 - Train Loss: 0.272770, Val Loss: 0.253237\n",
      "Epoch 11/30 - Train Loss: 0.269959, Val Loss: 0.200231\n",
      "Epoch 12/30 - Train Loss: 0.229210, Val Loss: 0.184055\n",
      "Epoch 13/30 - Train Loss: 0.221876, Val Loss: 0.180947\n",
      "Epoch 14/30 - Train Loss: 0.231144, Val Loss: 0.180649\n",
      "Epoch 15/30 - Train Loss: 0.208788, Val Loss: 0.184334\n",
      "Epoch 16/30 - Train Loss: 0.224197, Val Loss: 0.219918\n",
      "Epoch 17/30 - Train Loss: 0.236227, Val Loss: 0.193855\n",
      "Epoch 18/30 - Train Loss: 0.218711, Val Loss: 0.188739\n",
      "Epoch 19/30 - Train Loss: 0.225352, Val Loss: 0.190780\n",
      "Epoch 20/30 - Train Loss: 0.221945, Val Loss: 0.248619\n",
      "Epoch 21/30 - Train Loss: 0.235386, Val Loss: 0.192828\n",
      "Epoch 22/30 - Train Loss: 0.218454, Val Loss: 0.197128\n",
      "Epoch 23/30 - Train Loss: 0.198821, Val Loss: 0.190409\n",
      "Epoch 24/30 - Train Loss: 0.204497, Val Loss: 0.184047\n",
      "Epoch 25/30 - Train Loss: 0.220362, Val Loss: 0.196290\n",
      "Epoch 26/30 - Train Loss: 0.201902, Val Loss: 0.187184\n",
      "Epoch 27/30 - Train Loss: 0.185368, Val Loss: 0.185010\n",
      "Epoch 28/30 - Train Loss: 0.181399, Val Loss: 0.181821\n",
      "Epoch 29/30 - Train Loss: 0.181332, Val Loss: 0.177479\n",
      "Epoch 30/30 - Train Loss: 0.177377, Val Loss: 0.177543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading config.yaml; uploading output.log; uploading model_fold_3.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading model_fold_3.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate â–‡â–†â–ƒâ–‚â–ˆâ–ˆâ–‡â–‡â–†â–„â–ƒâ–‚â–‚â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss â–ˆâ–…â–„â–ƒâ–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss â–ˆâ–ˆâ–…â–„â–ƒâ–‚â–‚â–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–‚â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 0.17738\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.17754\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33mdecent-sponge-13\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/bnl6rka4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250330_024513-bnl6rka4/logs\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 4 validation NLL score: 25.259751\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.1\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250330_024915-djtmi3b4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33msplendid-puddle-14\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run at \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/djtmi3b4\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W&B successfully initialized\n",
      "Training fold 5...\n",
      "Epoch 1/30 - Train Loss: 1.769279, Val Loss: 1.012632\n",
      "Epoch 2/30 - Train Loss: 1.022569, Val Loss: 0.981403\n",
      "Epoch 3/30 - Train Loss: 0.876269, Val Loss: 0.657385\n",
      "Epoch 4/30 - Train Loss: 0.653066, Val Loss: 0.462843\n",
      "Epoch 5/30 - Train Loss: 0.563500, Val Loss: 0.420290\n",
      "Epoch 6/30 - Train Loss: 0.497858, Val Loss: 0.335038\n",
      "Epoch 7/30 - Train Loss: 0.380715, Val Loss: 0.253722\n",
      "Epoch 8/30 - Train Loss: 0.338781, Val Loss: 0.206428\n",
      "Epoch 9/30 - Train Loss: 0.283215, Val Loss: 0.179406\n",
      "Epoch 10/30 - Train Loss: 0.260626, Val Loss: 0.173059\n",
      "Epoch 11/30 - Train Loss: 0.255843, Val Loss: 0.162185\n",
      "Epoch 12/30 - Train Loss: 0.236991, Val Loss: 0.157004\n",
      "Epoch 13/30 - Train Loss: 0.242324, Val Loss: 0.155101\n",
      "Epoch 14/30 - Train Loss: 0.228522, Val Loss: 0.152222\n",
      "Epoch 15/30 - Train Loss: 0.223941, Val Loss: 0.150886\n",
      "Epoch 16/30 - Train Loss: 0.239145, Val Loss: 0.174325\n",
      "Epoch 17/30 - Train Loss: 0.241660, Val Loss: 0.156169\n",
      "Epoch 18/30 - Train Loss: 0.242059, Val Loss: 0.177404\n",
      "Epoch 19/30 - Train Loss: 0.237046, Val Loss: 0.190541\n",
      "Epoch 20/30 - Train Loss: 0.253317, Val Loss: 0.166465\n",
      "Epoch 21/30 - Train Loss: 0.221204, Val Loss: 0.162719\n",
      "Epoch 22/30 - Train Loss: 0.221782, Val Loss: 0.157367\n",
      "Epoch 23/30 - Train Loss: 0.218541, Val Loss: 0.168010\n",
      "Epoch 24/30 - Train Loss: 0.225669, Val Loss: 0.169788\n",
      "Epoch 25/30 - Train Loss: 0.223291, Val Loss: 0.147553\n",
      "Epoch 26/30 - Train Loss: 0.207151, Val Loss: 0.145205\n",
      "Epoch 27/30 - Train Loss: 0.200743, Val Loss: 0.146290\n",
      "Epoch 28/30 - Train Loss: 0.203857, Val Loss: 0.152150\n",
      "Epoch 29/30 - Train Loss: 0.193020, Val Loss: 0.148059\n",
      "Epoch 30/30 - Train Loss: 0.200255, Val Loss: 0.147433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading config.yaml; uploading output.log; uploading model_fold_4.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: uploading model_fold_4.pt\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:                                                                                \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch â–â–â–â–‚â–‚â–‚â–‚â–ƒâ–ƒâ–ƒâ–ƒâ–„â–„â–„â–„â–…â–…â–…â–…â–†â–†â–†â–†â–‡â–‡â–‡â–‡â–ˆâ–ˆâ–ˆ\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate â–‡â–†â–ƒâ–‚â–ˆâ–ˆâ–‡â–‡â–†â–„â–ƒâ–‚â–‚â–â–ˆâ–ˆâ–ˆâ–ˆâ–‡â–‡â–‡â–†â–†â–…â–„â–„â–ƒâ–ƒâ–‚â–‚\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss â–ˆâ–…â–„â–ƒâ–ƒâ–‚â–‚â–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss â–ˆâ–ˆâ–…â–„â–ƒâ–ƒâ–‚â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–â–\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:         epoch 30\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: learning_rate 7e-05\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:    train_loss 0.20026\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:      val_loss 0.14743\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: ðŸš€ View run \u001b[33msplendid-puddle-14\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge/runs/djtmi3b4\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: â­ï¸ View project at: \u001b[34m\u001b[4mhttps://wandb.ai/ml-samurai/geology-forecast-challenge\u001b[0m\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./wandb/run-20250330_024915-djtmi3b4/logs\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 5 validation NLL score: 23.074894\n"
     ]
    }
   ],
   "source": [
    "for fold_idx, (train_index, val_index) in enumerate(kf.split(X_num)):\n",
    "    test_preds = train_and_predict(\n",
    "        fold_idx, \n",
    "        train_index, \n",
    "        val_index, \n",
    "        X_num, \n",
    "        y,\n",
    "        X_num_test,\n",
    "        config\n",
    "    )\n",
    "    test_preds_all_folds[fold_idx] = test_preds\n",
    "    \n",
    "    fold_val_preds = train_sub.loc[val_index, ['geology_id'] + TARGETS]\n",
    "    fold_val_solution = solution.loc[val_index]\n",
    "    \n",
    "    fold_score = compute_nll_score(fold_val_solution, fold_val_preds)\n",
    "    val_scores.append(fold_score)\n",
    "    \n",
    "    print(f\"Fold {fold_idx+1} validation NLL score: {fold_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "faab4a76",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:53:18.925957Z",
     "iopub.status.busy": "2025-03-30T02:53:18.925644Z",
     "iopub.status.idle": "2025-03-30T02:53:18.930157Z",
     "shell.execute_reply": "2025-03-30T02:53:18.929324Z"
    },
    "papermill": {
     "duration": 0.020316,
     "end_time": "2025-03-30T02:53:18.931437",
     "exception": false,
     "start_time": "2025-03-30T02:53:18.911121",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average validation NLL score: 25.371141\n"
     ]
    }
   ],
   "source": [
    "avg_val_score = np.mean(val_scores)\n",
    "print(f\"Average validation NLL score: {avg_val_score:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "717f2e22",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:53:18.960300Z",
     "iopub.status.busy": "2025-03-30T02:53:18.960066Z",
     "iopub.status.idle": "2025-03-30T02:53:18.975281Z",
     "shell.execute_reply": "2025-03-30T02:53:18.974445Z"
    },
    "papermill": {
     "duration": 0.031045,
     "end_time": "2025-03-30T02:53:18.976595",
     "exception": false,
     "start_time": "2025-03-30T02:53:18.945550",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_preds_avg = np.mean(test_preds_all_folds, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "30a07ea0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:53:19.004817Z",
     "iopub.status.busy": "2025-03-30T02:53:19.004609Z",
     "iopub.status.idle": "2025-03-30T02:53:19.322889Z",
     "shell.execute_reply": "2025-03-30T02:53:19.322262Z"
    },
    "papermill": {
     "duration": 0.333922,
     "end_time": "2025-03-30T02:53:19.324453",
     "exception": false,
     "start_time": "2025-03-30T02:53:18.990531",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "submission = sub.copy()\n",
    "submission[TARGETS] = test_preds_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "560b5fe0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-30T02:53:19.353372Z",
     "iopub.status.busy": "2025-03-30T02:53:19.353146Z",
     "iopub.status.idle": "2025-03-30T02:53:22.426623Z",
     "shell.execute_reply": "2025-03-30T02:53:22.425802Z"
    },
    "papermill": {
     "duration": 3.089152,
     "end_time": "2025-03-30T02:53:22.428039",
     "exception": false,
     "start_time": "2025-03-30T02:53:19.338887",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file created successfully!\n"
     ]
    }
   ],
   "source": [
    "submission.to_csv('submission.csv', index=False)\n",
    "print(\"Submission file created successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11372669,
     "sourceId": 95697,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 30918,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1238.289722,
   "end_time": "2025-03-30T02:53:24.366149",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-03-30T02:32:46.076427",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
