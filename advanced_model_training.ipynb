{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from tqdm.auto import tqdm\n",
    "from scipy.special import logsumexp\n",
    "from collections import OrderedDict\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.ndimage import gaussian_filter1d\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import wandb\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_wandb(project_name=\"geology-forecast-challenge-sweep-gpu-bayes-30-server-HybridCNNLSTM\", config=None):\n",
    "    # Если run уже существует, просто возвращаем его\n",
    "    if wandb.run is not None:\n",
    "        return wandb.run\n",
    "    \n",
    "    try:\n",
    "        wandb_api_key = os.environ['WANDB_API_KEY']\n",
    "        \n",
    "        wandb.login(key=wandb_api_key)\n",
    "        \n",
    "        run = wandb.init(\n",
    "            project=project_name,\n",
    "            config=config,\n",
    "            tags=[\"LSTM\", \"Geology Forecast Challenge\", \"Feature Engineering\"],\n",
    "            reinit=True\n",
    "        )\n",
    "        \n",
    "        print(\"W&B successfully initialized\")\n",
    "        return run\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Error initializing W&B: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"data/train.csv\").fillna(0)\n",
    "test = pd.read_csv(\"data/test.csv\").fillna(0)\n",
    "sub = pd.read_csv('data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES = [c for c in test.columns if c != 'geology_id']\n",
    "TARGETS = [c for c in sub.columns if c != 'geology_id']\n",
    "solution = train[['geology_id'] + TARGETS].copy()\n",
    "train_sub = train[['geology_id'] + TARGETS].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(data, is_test=False):\n",
    "    feature_data = pd.DataFrame({'geology_id': data['geology_id']})\n",
    "    \n",
    "    historical_cols = [col for col in data.columns if col != 'geology_id' and col.startswith('-') or col == '0']\n",
    "    \n",
    "    historical_cols.sort(key=lambda x: int(x) if x.isdigit() else int(x))\n",
    "    \n",
    "    historical_data = data[historical_cols].values\n",
    "    \n",
    "    # 1. Calculate local slopes (first derivative)\n",
    "    slopes = np.zeros_like(historical_data)\n",
    "    for i in range(1, historical_data.shape[1]):\n",
    "        slopes[:, i] = historical_data[:, i] - historical_data[:, i-1]\n",
    "    \n",
    "    # 2. Calculate curvature (second derivative)\n",
    "    curvature = np.zeros_like(historical_data)\n",
    "    for i in range(1, slopes.shape[1]-1):\n",
    "        curvature[:, i] = slopes[:, i+1] - slopes[:, i]\n",
    "    \n",
    "    # 3. Create rolling statistics for last N points\n",
    "    window_sizes = [5, 10, 20]\n",
    "    for window in window_sizes:\n",
    "\n",
    "        if historical_data.shape[1] >= window:\n",
    "            \n",
    "            feature_data[f'mean_last_{window}'] = np.mean(historical_data[:, -window:], axis=1)\n",
    "            feature_data[f'std_last_{window}'] = np.std(historical_data[:, -window:], axis=1)\n",
    "\n",
    "            x = np.arange(window)\n",
    "            \n",
    "            for i in range(historical_data.shape[0]):\n",
    "                \n",
    "                y = historical_data[i, -window:]\n",
    "                \n",
    "                if np.all(y == 0):\n",
    "                    feature_data.loc[i, f'trend_last_{window}'] = 0\n",
    "                else:\n",
    "                    slope = np.polyfit(x, y, 1)[0]\n",
    "                    feature_data.loc[i, f'trend_last_{window}'] = slope\n",
    "    \n",
    "    # 4. Calculate smoothed versions of data (different levels of smoothing)\n",
    "    smooth_windows = [3, 5, 9]\n",
    "    for window in smooth_windows:\n",
    "        # Savitzky-Golay filter requires window_length > polyorder\n",
    "        if window <= 3: \n",
    "            continue\n",
    "        \n",
    "        if historical_data.shape[1] >= window:\n",
    "            sg_window = window if window % 2 == 1 else window + 1\n",
    "            \n",
    "            for i in range(historical_data.shape[0]):\n",
    "                data_slice = historical_data[i, -50:]\n",
    "                if len(data_slice) >= sg_window:\n",
    "                    try:\n",
    "                        polyorder = 2 if sg_window <= 5 else 3\n",
    "                        smoothed = savgol_filter(data_slice, sg_window, polyorder, mode='nearest')\n",
    "                        feature_data.loc[i, f'sg_smooth_{window}'] = smoothed[-1]\n",
    "                        \n",
    "                        if len(smoothed) >= 3:\n",
    "                            feature_data.loc[i, f'sg_smooth_slope_{window}'] = smoothed[-1] - smoothed[-2]\n",
    "                    except Exception as e:\n",
    "                        feature_data.loc[i, f'sg_smooth_{window}'] = data_slice[-1]\n",
    "                        if len(data_slice) >= 3:\n",
    "                            feature_data.loc[i, f'sg_smooth_slope_{window}'] = data_slice[-1] - data_slice[-2]\n",
    "                else:\n",
    "                    feature_data.loc[i, f'sg_smooth_{window}'] = historical_data[i, -1] if historical_data.shape[1] > 0 else 0\n",
    "                    feature_data.loc[i, f'sg_smooth_slope_{window}'] = 0\n",
    "    \n",
    "    # 5. Calculate frequency-domain features (FFT-based)\n",
    "    if historical_data.shape[1] >= 32: \n",
    "        for i in range(historical_data.shape[0]):\n",
    "            fft_vals = np.abs(np.fft.rfft(historical_data[i, -32:]))\n",
    "            feature_data.loc[i, 'dominant_freq'] = np.argmax(fft_vals[1:]) + 1 if len(fft_vals) > 1 else 0\n",
    "            feature_data.loc[i, 'dominant_power'] = np.max(fft_vals[1:]) if len(fft_vals) > 1 else 0\n",
    "            feature_data.loc[i, 'total_power'] = np.sum(fft_vals[1:]) if len(fft_vals) > 1 else 0\n",
    "    \n",
    "    # 6. Detect potential fault indicators\n",
    "    if historical_data.shape[1] >= 5:\n",
    "        max_changes = []\n",
    "        for i in range(historical_data.shape[0]):\n",
    "            max_change = 0\n",
    "            for j in range(historical_data.shape[1] - 5):\n",
    "                change = np.max(historical_data[i, j:j+5]) - np.min(historical_data[i, j:j+5])\n",
    "                max_change = max(max_change, change)\n",
    "            max_changes.append(max_change)\n",
    "        feature_data['max_change_5pt'] = max_changes\n",
    "    \n",
    "    # 7. Calculate geological dip angle features\n",
    "    if historical_data.shape[1] >= 10:\n",
    "        dips = []\n",
    "        for i in range(historical_data.shape[0]):\n",
    "            # Use linear regression to find dip angle\n",
    "            x = np.arange(10)\n",
    "            y = historical_data[i, -10:]\n",
    "            slope = np.polyfit(x, y, 1)[0]\n",
    "            # Convert to degrees (slope is rise/run, arctangent gives angle)\n",
    "            dip_angle = np.degrees(np.arctan(slope))\n",
    "            dips.append(dip_angle)\n",
    "        feature_data['dip_angle'] = dips\n",
    "    \n",
    "    # 8. Add the raw historical data (last 50 points)\n",
    "    for i in range(min(50, historical_data.shape[1])):\n",
    "        feature_data[f'raw_{i}'] = historical_data[:, -(i+1)]\n",
    "    \n",
    "    # 9. Create interaction features from important raw features\n",
    "    if 'mean_last_5' in feature_data.columns and 'trend_last_10' in feature_data.columns:\n",
    "        feature_data['mean_trend_interaction'] = feature_data['mean_last_5'] * feature_data['trend_last_10']\n",
    "    \n",
    "    if 'dip_angle' in feature_data.columns and 'max_change_5pt' in feature_data.columns:\n",
    "        feature_data['dip_change_interaction'] = feature_data['dip_angle'] * feature_data['max_change_5pt']\n",
    "\n",
    "    # 10. Non-linear transformations of important features\n",
    "    for col in feature_data.columns:\n",
    "        if col != 'geology_id' and not col.startswith('raw_'):\n",
    "            feature_data[f'{col}_squared'] = feature_data[col] ** 2\n",
    "            # Log transform for any potentially positive-only features\n",
    "            if np.all(feature_data[col] > 0):\n",
    "                feature_data[f'{col}_log'] = np.log1p(feature_data[col])\n",
    "    \n",
    "    # Add the original features as well\n",
    "    for col in historical_cols:\n",
    "        feature_data[col] = data[col]\n",
    "    \n",
    "    return feature_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTMForecastModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size,\n",
    "        num_layers,\n",
    "        output_size,\n",
    "        dropout=0.2,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if num_layers > 1 else 0,\n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(hidden_size)\n",
    "        \n",
    "        self.fc1 = nn.Linear(hidden_size, hidden_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "        self.activation = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "        \n",
    "        x = self.fc1(lstm_out)\n",
    "        x = self.activation(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerForecastModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        d_model=512,\n",
    "        nhead=8,\n",
    "        num_layers=4,\n",
    "        dim_feedforward=2048,\n",
    "        dropout=0.1,\n",
    "        output_size=300\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_size, d_model)\n",
    "        \n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        encoder_layers = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model, \n",
    "            nhead=nhead, \n",
    "            dim_feedforward=dim_feedforward, \n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers, num_layers)\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(d_model, d_model // 2),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_model // 2, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        x = self.pos_encoder(x)\n",
    "        \n",
    "        transformer_output = self.transformer_encoder(x)\n",
    "\n",
    "        output = self.output_layer(transformer_output[:, -1, :])\n",
    "        \n",
    "        return output\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        position = torch.arange(max_len).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * (-math.log(10000.0) / d_model))\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(1)]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, dilation, dropout=0.2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Padding для сохранения размерности\n",
    "        padding = (kernel_size - 1) * dilation\n",
    "        \n",
    "        self.conv = nn.Conv1d(\n",
    "            in_channels, \n",
    "            out_channels, \n",
    "            kernel_size, \n",
    "            padding=padding, \n",
    "            dilation=dilation\n",
    "        )\n",
    "        \n",
    "        self.relu = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.residual = nn.Conv1d(in_channels, out_channels, 1) if in_channels != out_channels else nn.Identity()\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(out_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = self.residual(x)\n",
    "        \n",
    "        out = self.conv(x)\n",
    "        \n",
    "        out = out[:, :, :-self.conv.padding[0]]\n",
    "        \n",
    "        out = out + residual\n",
    "        \n",
    "        out = self.relu(out)\n",
    "        out = self.dropout(out)\n",
    "        \n",
    "        out = out.permute(0, 2, 1)\n",
    "        out = self.layer_norm(out)\n",
    "        out = out.permute(0, 2, 1) \n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TCNForecastModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        hidden_size=128,\n",
    "        kernel_size=3,\n",
    "        num_layers=8,\n",
    "        dropout=0.2,\n",
    "        output_size=300\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.input_projection = nn.Linear(input_size, hidden_size)\n",
    "        \n",
    "        self.tcn_blocks = nn.ModuleList()\n",
    "        for i in range(num_layers):\n",
    "            dilation = 2 ** i  # 1, 2, 4, 8, ...\n",
    "            self.tcn_blocks.append(\n",
    "                TCNBlock(hidden_size, hidden_size, kernel_size, dilation, dropout)\n",
    "            )\n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_size, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.input_projection(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        for block in self.tcn_blocks:\n",
    "            x = block(x)\n",
    "        \n",
    "        x = x[:, :, -1]\n",
    "        \n",
    "        x = self.output_layer(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HybridCNNLSTMModel(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_size,\n",
    "        cnn_filters=[64, 128, 128, 256],\n",
    "        kernel_size=3,\n",
    "        lstm_hidden=512,\n",
    "        lstm_layers=2,\n",
    "        dropout=0.2,\n",
    "        output_size=300\n",
    "    ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.cnn_layers = nn.ModuleList()\n",
    "        \n",
    "        self.cnn_layers.append(nn.Conv1d(input_size, cnn_filters[0], kernel_size, padding=kernel_size//2))\n",
    "        \n",
    "        for i in range(1, len(cnn_filters)):\n",
    "            self.cnn_layers.append(\n",
    "                nn.Conv1d(cnn_filters[i-1], cnn_filters[i], kernel_size, padding=kernel_size//2)\n",
    "            )\n",
    "        \n",
    "        self.batch_norms = nn.ModuleList([\n",
    "            nn.BatchNorm1d(filters) for filters in cnn_filters\n",
    "        ])\n",
    "        \n",
    "        self.act = nn.GELU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=cnn_filters[-1],\n",
    "            hidden_size=lstm_hidden,\n",
    "            num_layers=lstm_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if lstm_layers > 1 else 0,\n",
    "            bidirectional=True \n",
    "        )\n",
    "        \n",
    "        self.layer_norm = nn.LayerNorm(lstm_hidden * 2) \n",
    "        \n",
    "        self.output_layer = nn.Sequential(\n",
    "            nn.Linear(lstm_hidden * 2, lstm_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(lstm_hidden, output_size)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        for i, (conv, bn) in enumerate(zip(self.cnn_layers, self.batch_norms)):\n",
    "            x = conv(x)\n",
    "            x = bn(x)\n",
    "            x = self.act(x)\n",
    "            x = self.dropout(x)\n",
    "        \n",
    "        x = x.permute(0, 2, 1)\n",
    "        \n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        lstm_out = lstm_out[:, -1, :]\n",
    "        \n",
    "        lstm_out = self.layer_norm(lstm_out)\n",
    "\n",
    "        x = self.output_layer(lstm_out)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GeologyDataset(Dataset):\n",
    "    def __init__(self, features, targets=None, is_test=False, scale_features=True):\n",
    "        self.features = features\n",
    "        self.targets = targets\n",
    "        self.is_test = is_test\n",
    "        \n",
    "        if scale_features:\n",
    "            self.feature_scaler = StandardScaler()\n",
    "            self.features = self.feature_scaler.fit_transform(self.features)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.features)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        x = self.features[idx]\n",
    "        \n",
    "        x = x.reshape(-1, 1)\n",
    "        \n",
    "        if self.is_test:\n",
    "            return x\n",
    "        else:\n",
    "            y = self.targets[idx]\n",
    "            return x, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_data(df, engineered_features=None, target_cols=None, is_test=False):\n",
    "    if engineered_features is None:\n",
    "        # If no engineered features provided, use raw data\n",
    "        feature_cols = [c for c in df.columns if c != 'geology_id' and (c.startswith('-') or c == '0')]\n",
    "        X = df[feature_cols].values\n",
    "    else:\n",
    "        # Use the engineered features, dropping the ID column\n",
    "        X = engineered_features.drop('geology_id', axis=1).values\n",
    "    \n",
    "    if not is_test:\n",
    "        y = df[target_cols].values\n",
    "        return X, y\n",
    "    \n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_nll_score(solution, submission, row_id_column_name='geology_id'):\n",
    "    solution_copy = solution.copy()\n",
    "    submission_copy = submission.copy()\n",
    "    \n",
    "    del solution_copy[row_id_column_name]\n",
    "    del submission_copy[row_id_column_name]\n",
    "\n",
    "    NEGATIVE_PART = -299\n",
    "    LARGEST_CHUNK = 600\n",
    "    SMALLEST_CHUNK = 350\n",
    "    TOTAL_REALIZATIONS = 10\n",
    "    INFLATION_SIGMA = 600\n",
    "    \n",
    "    sigma_2 = np.ones((LARGEST_CHUNK+NEGATIVE_PART-1))\n",
    "    from_ranges = [1, 61, 245]\n",
    "    to_ranges_excl = [61, 245, 301]\n",
    "    log_slopes = [1.0406028049510443, 0.0, 7.835345062351012]\n",
    "    log_offsets = [-6.430669850650689, -2.1617411566043896, -45.24876794412965]\n",
    "\n",
    "    for growth_mode in range(len(from_ranges)):\n",
    "        for i in range(from_ranges[growth_mode], to_ranges_excl[growth_mode]):\n",
    "            sigma_2[i-1] = np.exp(np.log(i)*log_slopes[growth_mode]+log_offsets[growth_mode])\n",
    "\n",
    "    sigma_2 *= INFLATION_SIGMA\n",
    "  \n",
    "    cov_matrix_inv_diag = 1. / sigma_2\n",
    "    \n",
    "    num_rows = solution_copy.shape[0]\n",
    "    num_columns = LARGEST_CHUNK + NEGATIVE_PART - 1\n",
    "    \n",
    "    p = 1./TOTAL_REALIZATIONS\n",
    "    log_p = np.log(p)\n",
    "    \n",
    "    solution_arr = np.zeros((num_rows, TOTAL_REALIZATIONS, num_columns))\n",
    "    submission_arr = np.zeros((num_rows, TOTAL_REALIZATIONS, num_columns))\n",
    "    \n",
    "    for k in range(TOTAL_REALIZATIONS):\n",
    "        for i in range(num_columns):\n",
    "            if k == 0:\n",
    "                column_name = str(i+1)\n",
    "            else:\n",
    "                column_name = f\"r_{k}_pos_{i+1}\"\n",
    "            solution_arr[:, k, i] = solution_copy[column_name].values\n",
    "            submission_arr[:, k, i] = submission_copy[column_name].values\n",
    "\n",
    "    misfit = solution_arr - submission_arr\n",
    "    inner_product_matrix = np.sum(cov_matrix_inv_diag * misfit * misfit, axis=2)\n",
    "    \n",
    "    nll = -logsumexp(log_p - inner_product_matrix, axis=1)\n",
    "    \n",
    "    return nll.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_with_nll_loss(model, train_loader, optimizer, device, epoch=0, total_epochs=30):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for data, target in pbar:\n",
    "        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.float32)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        \n",
    "        target_mean = target.mean(dim=0)\n",
    "        target_std = target.std(dim=0) + 1e-6\n",
    "        \n",
    "        normalized_output = (output - target_mean) / target_std\n",
    "        normalized_target = (target - target_mean) / target_std\n",
    "        \n",
    "        loss = F.mse_loss(normalized_output, normalized_target)\n",
    "        \n",
    "        if output.shape[1] > 1:\n",
    "            smoothness_penalty = torch.mean(torch.abs(output[:, 1:] - output[:, :-1]))\n",
    "            loss += 0.01 * smoothness_penalty\n",
    "        \n",
    "        loss.backward()\n",
    "\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.6f}\"})\n",
    "    \n",
    "    return np.mean(train_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(model, val_loader, device):\n",
    "    model.eval()\n",
    "    val_losses = []\n",
    "    val_preds = []\n",
    "    val_targets = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target in tqdm(val_loader, desc=\"Validating\"):\n",
    "            data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.float32)\n",
    "            output = model(data)\n",
    "            \n",
    "            target_mean = target.mean(dim=0)\n",
    "            target_std = target.std(dim=0) + 1e-6\n",
    "            \n",
    "            normalized_output = (output - target_mean) / target_std\n",
    "            normalized_target = (target - target_mean) / target_std\n",
    "            \n",
    "            loss = F.mse_loss(normalized_output, normalized_target)\n",
    "            \n",
    "            val_losses.append(loss.item())\n",
    "            val_preds.append(output.cpu().numpy())\n",
    "            val_targets.append(target.cpu().numpy())\n",
    "    \n",
    "    val_preds = np.concatenate(val_preds)\n",
    "    val_targets = np.concatenate(val_targets)\n",
    "    \n",
    "    return np.mean(val_losses), val_preds, val_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_diverse_realizations(base_predictions, num_samples=10, diversity_factor=0.2):\n",
    "    num_rows, num_cols = base_predictions.shape\n",
    "    realizations = np.zeros((num_samples, num_rows, num_cols))\n",
    "\n",
    "    realizations[0] = base_predictions\n",
    "\n",
    "    for i in range(1, num_samples):\n",
    "        realization = base_predictions.copy()\n",
    "        \n",
    "        for j in range(num_rows):\n",
    "            noise = np.random.normal(0, diversity_factor, num_cols)\n",
    "            \n",
    "            smoothed_noise = gaussian_filter1d(noise, sigma=5.0)\n",
    "            \n",
    "            position_factor = np.linspace(0.1, 1.0, num_cols)\n",
    "            scaled_noise = smoothed_noise * position_factor\n",
    "            \n",
    "            realization[j] += scaled_noise\n",
    "\n",
    "            for k in range(1, num_cols):\n",
    "\n",
    "                max_change = 2.0 * (k/num_cols + 0.1)  # Allow larger changes further away\n",
    "                diff = realization[j, k] - realization[j, k-1]\n",
    "                if abs(diff) > max_change:\n",
    "                    realization[j, k] = realization[j, k-1] + np.sign(diff) * max_change\n",
    "        \n",
    "        realizations[i] = realization\n",
    "    \n",
    "    return realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_fold_realizations(base_predictions, num_realizations=10):\n",
    "    realizations = generate_diverse_realizations(\n",
    "        base_predictions, \n",
    "        num_samples=num_realizations,\n",
    "        diversity_factor=0.15  # Control the diversity level\n",
    "    )\n",
    "    return realizations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_features(train_features, y):\n",
    "    feature_cols = [col for col in train_features.columns if col != 'geology_id']\n",
    "    \n",
    "    target_cols = [str(i) for i in range(1, 11)]\n",
    "    target_cols = [col for col in target_cols if col in train.columns]\n",
    "    \n",
    "    correlations = []\n",
    "    for tcol in target_cols:\n",
    "        if tcol in train.columns:\n",
    "            for fcol in feature_cols:\n",
    "                corr = np.corrcoef(train_features[fcol], train[tcol])[0, 1]\n",
    "                correlations.append((fcol, tcol, abs(corr)))\n",
    "    \n",
    "    top_correlations = sorted(correlations, key=lambda x: x[2], reverse=True)[:15]\n",
    "    \n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_data = pd.DataFrame(top_correlations, columns=['Feature', 'Target', 'Correlation'])\n",
    "    sns.barplot(data=plot_data, x='Correlation', y='Feature', hue='Target')\n",
    "    plt.title('Top Feature Correlations with Targets')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    try:\n",
    "        wandb.log({\"feature_correlations\": wandb.Image(plt)})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_default_config(model_type):    \n",
    "    if model_type == 'LSTM':\n",
    "        return {\n",
    "            'model_type': 'LSTM',\n",
    "            'hidden_size': 512,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.2,\n",
    "            'learning_rate': 5e-4,\n",
    "            'weight_decay': 1e-5,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 50,\n",
    "            'seed': SEED,\n",
    "            'feature_engineering': 'advanced',\n",
    "            'optimizer': 'adamw',\n",
    "            'scheduler': 'onecycle'\n",
    "        }\n",
    "    elif model_type == 'Transformer':\n",
    "        return {\n",
    "            'model_type': 'Transformer',\n",
    "            'd_model': 512,\n",
    "            'nhead': 8,\n",
    "            'num_layers': 4,\n",
    "            'dim_feedforward': 1024,\n",
    "            'dropout': 0.2,\n",
    "            'learning_rate': 4e-4,\n",
    "            'weight_decay': 1e-5,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 50,\n",
    "            'seed': SEED,\n",
    "            'feature_engineering': 'advanced',\n",
    "            'optimizer': 'adamw',\n",
    "            'scheduler': 'cosine'\n",
    "        }\n",
    "    elif model_type == 'TCN':\n",
    "        return {\n",
    "            'model_type': 'TCN',\n",
    "            'hidden_size': 256,\n",
    "            'kernel_size': 3,\n",
    "            'num_layers': 8,\n",
    "            'dropout': 0.2,\n",
    "            'learning_rate': 5e-4,\n",
    "            'weight_decay': 1e-5,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 50,\n",
    "            'seed': SEED,\n",
    "            'feature_engineering': 'advanced',\n",
    "            'optimizer': 'adam',\n",
    "            'scheduler': 'onecycle'\n",
    "        }\n",
    "    elif model_type == 'HybridCNNLSTM':\n",
    "        return {\n",
    "            'model_type': 'HybridCNNLSTM',\n",
    "            'cnn_filters': [64, 128, 128, 256],\n",
    "            'kernel_size': 3,\n",
    "            'hidden_size': 512,\n",
    "            'num_layers': 2,\n",
    "            'dropout': 0.3,\n",
    "            'learning_rate': 3e-4,\n",
    "            'weight_decay': 1e-5,\n",
    "            'batch_size': 128,\n",
    "            'epochs': 60,\n",
    "            'seed': SEED,\n",
    "            'feature_engineering': 'advanced',\n",
    "            'optimizer': 'adamw',\n",
    "            'scheduler': 'cosine'\n",
    "        }\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный тип модели: {model_type}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config, device):\n",
    "    model_type = config['model_type']\n",
    "    input_features_size = 1 \n",
    "    output_size = 3000\n",
    "    \n",
    "    if model_type == 'LSTM':\n",
    "        model = LSTMForecastModel(\n",
    "            input_size=input_features_size,\n",
    "            hidden_size=config['hidden_size'],\n",
    "            num_layers=config['num_layers'],\n",
    "            output_size=output_size,\n",
    "            dropout=config['dropout']\n",
    "        )\n",
    "    elif model_type == 'Transformer':\n",
    "        model = TransformerForecastModel(\n",
    "            input_size=input_features_size,\n",
    "            d_model=config.get('d_model', 512),\n",
    "            nhead=config.get('nhead', 8),\n",
    "            num_layers=config['num_layers'],\n",
    "            dim_feedforward=config.get('dim_feedforward', 2048),\n",
    "            dropout=config['dropout'],\n",
    "            output_size=output_size\n",
    "        )\n",
    "    elif model_type == 'TCN':\n",
    "        model = TCNForecastModel(\n",
    "            input_size=input_features_size,\n",
    "            hidden_size=config['hidden_size'],\n",
    "            kernel_size=config.get('kernel_size', 3),\n",
    "            num_layers=config['num_layers'],\n",
    "            dropout=config['dropout'],\n",
    "            output_size=output_size\n",
    "        )\n",
    "    elif model_type == 'HybridCNNLSTM':\n",
    "        model = HybridCNNLSTMModel(\n",
    "            input_size=input_features_size,\n",
    "            cnn_filters=config.get('cnn_filters', [64, 128, 128, 256]),\n",
    "            kernel_size=config.get('kernel_size', 3),\n",
    "            lstm_hidden=config['hidden_size'],\n",
    "            lstm_layers=config['num_layers'],\n",
    "            dropout=config['dropout'],\n",
    "            output_size=output_size\n",
    "        )\n",
    "    else:\n",
    "        raise ValueError(f\"Неизвестный тип модели: {model_type}\")\n",
    "    \n",
    "    return model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_predict_single_model(\n",
    "    X_train, \n",
    "    y_train,\n",
    "    X_val,\n",
    "    y_val,\n",
    "    X_test,\n",
    "    config,\n",
    "    run_name=None,\n",
    "    autofinish=True,\n",
    "    wandb_run=None\n",
    "):\n",
    "    try:\n",
    "        # Используем переданный wandb_run или инициализируем новый\n",
    "        run = wandb_run if wandb_run is not None else init_wandb(project_name=run_name, config=config)\n",
    "        \n",
    "        train_dataset = GeologyDataset(X_train, y_train)\n",
    "        val_dataset = GeologyDataset(X_val, y_val)\n",
    "        test_dataset = GeologyDataset(X_test, is_test=True)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config['batch_size'], \n",
    "            shuffle=True,\n",
    "            pin_memory=True, \n",
    "            num_workers=2  \n",
    "        )\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=config['batch_size'], \n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=2\n",
    "        )\n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=config['batch_size'], \n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=2\n",
    "        )\n",
    "        \n",
    "        model = create_model(config, device)\n",
    "        \n",
    "        if config.get('optimizer', 'adamw').lower() == 'adamw':\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=config['learning_rate'],\n",
    "                weight_decay=config['weight_decay'],\n",
    "                eps=1e-8\n",
    "            )\n",
    "        elif config.get('optimizer', 'adamw').lower() == 'adam':\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=config['learning_rate'],\n",
    "                weight_decay=config['weight_decay'],\n",
    "                eps=1e-8\n",
    "            )\n",
    "        elif config.get('optimizer', 'adamw').lower() == 'sgd':\n",
    "            optimizer = optim.SGD(\n",
    "                model.parameters(),\n",
    "                lr=config['learning_rate'],\n",
    "                momentum=config.get('momentum', 0.9),\n",
    "                weight_decay=config['weight_decay']\n",
    "            )\n",
    "        \n",
    "        if config.get('scheduler', 'onecycle').lower() == 'onecycle':\n",
    "            steps_per_epoch = len(train_loader)\n",
    "            scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=config['learning_rate'],\n",
    "                epochs=config['epochs'],\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                pct_start=0.3,\n",
    "                div_factor=25,\n",
    "                final_div_factor=1000,\n",
    "            )\n",
    "        elif config.get('scheduler', 'onecycle').lower() == 'cosine':\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, \n",
    "                T_max=config['epochs'] // 2,\n",
    "                eta_min=config['learning_rate'] / 1000\n",
    "            )\n",
    "        elif config.get('scheduler', 'onecycle').lower() == 'reduce':\n",
    "            scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "                optimizer,\n",
    "                mode='min',\n",
    "                factor=0.5,\n",
    "                patience=5,\n",
    "                min_lr=config['learning_rate'] / 100\n",
    "            )\n",
    "        \n",
    "        best_val_loss = float('inf')\n",
    "        val_predictions = None\n",
    "        \n",
    "        # early stopping\n",
    "        patience = 10\n",
    "        counter = 0\n",
    "        min_delta = 1e-4\n",
    "        \n",
    "        print(f\"Training model {config['model_type']}...\")\n",
    "        for epoch in range(config['epochs']):\n",
    "            train_loss = train_model_with_nll_loss(\n",
    "                model, train_loader, optimizer, device, epoch, config['epochs']\n",
    "            )\n",
    "            \n",
    "            val_loss, val_preds, val_targets = validate_model(model, val_loader, device)\n",
    "            \n",
    "            val_predictions = val_preds\n",
    "            \n",
    "            if config.get('scheduler', 'onecycle').lower() in ['onecycle', 'cosine']:\n",
    "                scheduler.step()\n",
    "            elif config.get('scheduler', 'onecycle').lower() == 'reduce':\n",
    "                scheduler.step(val_loss)\n",
    "    \n",
    "            if run:\n",
    "                run.log({\n",
    "                    \"epoch\": epoch + 1,\n",
    "                    \"train_loss\": train_loss,\n",
    "                    \"val_loss\": val_loss,\n",
    "                    \"learning_rate\": optimizer.param_groups[0]['lr']\n",
    "                })\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{config['epochs']} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}\")\n",
    "            \n",
    "            # loss check\n",
    "            if epoch + 1 == 12 and val_loss > 1.0:\n",
    "                print(f\"Validation loss > 1.0 at epoch 10 ({val_loss:.6f}). Stopping training.\")\n",
    "                if run:\n",
    "                    run.log({\"early_stop_reason\": \"high_loss_at_epoch_10\", \"best_val_loss\": best_val_loss})\n",
    "                break\n",
    "            \n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                model_path = f\"model_{config['model_type']}.pt\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                if run:\n",
    "                    run.save(model_path)\n",
    "                print(f\"Saved new best model with validation loss: {val_loss:.6f}\")\n",
    "                counter = 0 \n",
    "            else:\n",
    "                if val_loss > best_val_loss - min_delta:\n",
    "                    counter += 1\n",
    "                    if counter >= patience:\n",
    "                        print(f\"Early stopping triggered at epoch {epoch+1}. Best val_loss: {best_val_loss:.6f}\")\n",
    "                        if run:\n",
    "                            run.log({\"early_stop_reason\": \"no_improvement\", \"best_val_loss\": best_val_loss})\n",
    "                        break\n",
    "        \n",
    "        model.load_state_dict(torch.load(f\"model_{config['model_type']}.pt\"))\n",
    "        model.eval()\n",
    "        test_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_loader, desc=\"Predicting test data\"):\n",
    "                if isinstance(data, list):\n",
    "                    data = data[0]\n",
    "                data = data.to(device, dtype=torch.float32)\n",
    "                output = model(data)\n",
    "                test_preds.append(output.cpu().numpy())\n",
    "        \n",
    "        base_test_predictions = np.concatenate(test_preds)\n",
    "        \n",
    "        val_preds_df = pd.DataFrame(\n",
    "            data=val_predictions,\n",
    "            columns=TARGETS,\n",
    "        )\n",
    "        val_preds_df['geology_id'] = val_dataset.features[:, 0]\n",
    "        \n",
    "        val_solution_df = pd.DataFrame(\n",
    "            data=y_val,\n",
    "            columns=TARGETS,\n",
    "        )\n",
    "        val_solution_df['geology_id'] = val_dataset.features[:, 0]\n",
    "        \n",
    "        nll_score = compute_nll_score(val_solution_df, val_preds_df)\n",
    "        \n",
    "        # Завершаем run только если мы его создали и autofinish=True\n",
    "        if run and autofinish and wandb_run is None:\n",
    "            run.log({\"val_nll_score\": nll_score})\n",
    "            run.finish()\n",
    "        \n",
    "        return base_test_predictions, val_predictions, nll_score\n",
    "        \n",
    "    finally:\n",
    "        torch.cuda.empty_cache()\n",
    "        \n",
    "        from torch.utils.data import _utils\n",
    "        if hasattr(_utils.worker, \"_worker_pool\"):\n",
    "            _utils.worker._worker_pool.close()\n",
    "            _utils.worker._worker_pool.join()\n",
    "        elif hasattr(_utils.worker, \"_shutdown_all_workers\"):\n",
    "            _utils.worker._shutdown_all_workers()\n",
    "        \n",
    "        import gc\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_submission(base_predictions, config, val_nll_score):\n",
    "    submission = sub.copy()\n",
    "    \n",
    "    for i in range(300):\n",
    "        col_name = str(i+1)\n",
    "        submission[col_name] = base_predictions[:, i]\n",
    "    \n",
    "    realizations = generate_diverse_realizations(\n",
    "        base_predictions, \n",
    "        num_samples=10, \n",
    "        diversity_factor=0.15 + 0.05 * (config.get('diversity_factor', 1.0) - 1.0)\n",
    "    )\n",
    "    \n",
    "    for r_idx in range(1, 10): \n",
    "        for i in range(300):\n",
    "            col_name = f\"r_{r_idx}_pos_{i+1}\"\n",
    "            submission[col_name] = realizations[r_idx][:, i]\n",
    "    \n",
    "    submission_file = f\"submission_{config['model_type']}_{val_nll_score:.6f}.csv\"\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"\\nSubmission file saved: {submission_file}\")\n",
    "    \n",
    "    expected_cols = sub.columns.tolist()\n",
    "    actual_cols = submission.columns.tolist()\n",
    "    \n",
    "    if set(expected_cols) != set(actual_cols):\n",
    "        print(\"WARNING: Submission columns don't match expected format!\")\n",
    "        missing = set(expected_cols) - set(actual_cols)\n",
    "        extra = set(actual_cols) - set(expected_cols)\n",
    "        if missing:\n",
    "            print(f\"Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"Extra columns: {extra}\")\n",
    "    else:\n",
    "        print(\"Submission format validated successfully!\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model_predictions(submission, config, run=None):\n",
    "    try:\n",
    "        plt.figure(figsize=(15, 8))\n",
    "\n",
    "        sample_indices = [0, 10, 20]\n",
    "        \n",
    "        for sample_idx in sample_indices:\n",
    "            plt.figure(figsize=(15, 8))\n",
    "            \n",
    "            x_coords = np.arange(1, 301)\n",
    "            plt.plot(x_coords, submission.iloc[sample_idx, 1:301].values, \n",
    "                    color='blue', label='Baseline', linewidth=2)\n",
    "            \n",
    "            colors = ['red', 'green', 'purple']\n",
    "            for r in range(1, 4): \n",
    "                cols = [f\"r_{r}_pos_{i+1}\" for i in range(300)]\n",
    "                plt.plot(x_coords, submission.loc[submission.index[sample_idx], cols].values,\n",
    "                        color=colors[(r-1) % len(colors)], label=f'Realization {r}', alpha=0.7)\n",
    "            \n",
    "            plt.title(f\"{config['model_type']} - Sample {sample_idx}\")\n",
    "            plt.xlabel(\"Position\")\n",
    "            plt.ylabel(\"Layer Depth (Z coordinate)\")\n",
    "            plt.legend()\n",
    "            plt.grid(True, alpha=0.3)\n",
    "\n",
    "            if run:\n",
    "                run.log({f\"predictions_sample_{sample_idx}\": wandb.Image(plt)})\n",
    "            \n",
    "            plt.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Visualization error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_train_val(X_features, y, val_size=0.2, random_state=42):\n",
    "    n_samples = len(X_features)\n",
    "    indices = np.arange(n_samples)\n",
    "    np.random.seed(random_state)\n",
    "    np.random.shuffle(indices)\n",
    "    \n",
    "    val_samples = int(val_size * n_samples)\n",
    "    val_indices = indices[:val_samples]\n",
    "    train_indices = indices[val_samples:]\n",
    "    \n",
    "    X_train = X_features[train_indices]\n",
    "    y_train = y[train_indices]\n",
    "    X_val = X_features[val_indices]\n",
    "    y_val = y[val_indices]\n",
    "    \n",
    "    return X_train, y_train, X_val, y_val, train_indices, val_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(config, autofinish=True, wandb_run=None):\n",
    "    try:\n",
    "        seed_everything(config['seed'])\n",
    "        \n",
    "        print(f\"\\n{'='*50}\\nRunning experiment with {config['model_type']}\\n{'='*50}\")\n",
    "        print(f\"Config: {config}\")\n",
    "        \n",
    "        X_train, y_train, X_val, y_val, train_indices, val_indices = split_train_val(\n",
    "            X_features, y, val_size=0.2, random_state=config['seed']\n",
    "        )\n",
    "\n",
    "        base_predictions, val_predictions, val_nll_score = train_and_predict_single_model(\n",
    "        X_train, y_train, X_val, y_val, X_features_test, config, \n",
    "        run_name=\"geology-forecast-challenge\", \n",
    "        autofinish=autofinish, wandb_run=wandb_run)\n",
    "        \n",
    "        train_sub.loc[val_indices, TARGETS] = val_predictions\n",
    "        \n",
    "        submission = create_submission(base_predictions, config, val_nll_score)\n",
    "    \n",
    "        visualize_model_predictions(submission, config)\n",
    "        \n",
    "        print(f\"Finished experiment with {config['model_type']}, validation NLL: {val_nll_score:.6f}\")\n",
    "        \n",
    "        return val_nll_score\n",
    "    \n",
    "    except RuntimeError as e:\n",
    "        if \"CUDA out of memory\" in str(e):\n",
    "            print(f\"CUDA OOM error with config: {config}\")\n",
    "            \n",
    "            if wandb.run is not None:\n",
    "                wandb.log({\"cuda_oom_error\": True, \"error_message\": str(e)})\n",
    "                \n",
    "            # Большое значение метрики, чтобы байесовский оптимизатор избегал таких конфигураций\n",
    "            return float('inf')\n",
    "        else:\n",
    "            raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_predictions(test_idx=0, num_realizations=3):\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    x_coords = np.arange(1, 301)\n",
    "    plt.plot(x_coords, submission.iloc[test_idx, 1:301].values, \n",
    "             color='blue', label='Realization 0', linewidth=2)\n",
    "    \n",
    "    colors = ['red', 'green', 'purple']\n",
    "    for r in range(1, min(num_realizations+1, 10)):\n",
    "        cols = [f\"r_{r}_pos_{i+1}\" for i in range(300)]\n",
    "        plt.plot(x_coords, submission.loc[submission.index[test_idx], cols].values,\n",
    "                color=colors[(r-1) % len(colors)], label=f'Realization {r}', alpha=0.7)\n",
    "    \n",
    "    plt.title(f\"Multiple Geological Sequence Realizations for Sample {test_idx}\")\n",
    "    plt.xlabel(\"Position\")\n",
    "    plt.ylabel(\"Layer Depth (Z coordinate)\")\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    try:\n",
    "        wandb.log({\"prediction_visualization\": wandb.Image(plt)})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_geological_patterns(submission_df, sample_indices=None, num_samples=5):\n",
    "    if sample_indices is None:\n",
    "        sample_indices = np.random.choice(len(submission_df), min(num_samples, len(submission_df)), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(18, 12))\n",
    "    \n",
    "    for idx, i in enumerate(sample_indices):\n",
    "        plt.subplot(len(sample_indices), 1, idx+1)\n",
    "        \n",
    "        base_pred = submission_df.iloc[i, 1:301].values\n",
    "        \n",
    "        slopes = np.diff(base_pred)\n",
    "        \n",
    "        threshold = np.std(slopes) * 2.5\n",
    "        fault_indicators = np.where(np.abs(slopes) > threshold)[0]\n",
    "        \n",
    "        plt.plot(range(1, 301), base_pred, 'b-', linewidth=2, label='Predicted Sequence')\n",
    "        \n",
    "        if len(fault_indicators) > 0:\n",
    "            plt.scatter([x+1 for x in fault_indicators], \n",
    "                       [base_pred[x] for x in fault_indicators],\n",
    "                       color='red', s=80, marker='x', label='Potential Fault/Change')\n",
    "        \n",
    "        segment_size = 50\n",
    "        for seg_start in range(0, 300, segment_size):\n",
    "            seg_end = min(seg_start + segment_size, 300)\n",
    "            if seg_end - seg_start > 10:  # Only fit if enough points\n",
    "                x_seg = np.arange(seg_start, seg_end)\n",
    "                y_seg = base_pred[seg_start:seg_end]\n",
    "                # Fit a line to this segment\n",
    "                z = np.polyfit(x_seg, y_seg, 1)\n",
    "                p = np.poly1d(z)\n",
    "                plt.plot(x_seg+1, p(x_seg), '--', linewidth=1.5, \n",
    "                         alpha=0.7, label=f'Trend (Seg {seg_start}-{seg_end})')\n",
    "        \n",
    "        plt.title(f\"Geological Analysis for Sample {i}\")\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.ylabel(\"Layer Depth (Z)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        if idx == 0:\n",
    "            plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "\n",
    "    try:\n",
    "        wandb.log({\"geological_analysis\": wandb.Image(plt)})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_prediction_uncertainty(submission_df, num_samples=5):\n",
    "    sample_indices = np.random.choice(len(submission_df), min(num_samples, len(submission_df)), replace=False)\n",
    "    \n",
    "    plt.figure(figsize=(15, 10))\n",
    "    \n",
    "    for idx, i in enumerate(sample_indices):\n",
    "        plt.subplot(num_samples, 1, idx+1)\n",
    "        \n",
    "        realizations = []\n",
    "        \n",
    "        base_realization = submission_df.iloc[i, 1:301].values\n",
    "        realizations.append(base_realization)\n",
    "        \n",
    "        for r in range(1, 10):\n",
    "            cols = [f\"r_{r}_pos_{i+1}\" for i in range(300)]\n",
    "            realization = submission_df.loc[submission_df.index[i], cols].values\n",
    "            realizations.append(realization)\n",
    "        \n",
    "        realizations = np.array(realizations)\n",
    "        \n",
    "        mean_prediction = np.mean(realizations, axis=0)\n",
    "        std_prediction = np.std(realizations, axis=0)\n",
    "        \n",
    "        x_coords = np.arange(1, 301)\n",
    "        plt.plot(x_coords, mean_prediction, 'b-', label='Mean Prediction')\n",
    "        \n",
    "        plt.fill_between(x_coords, \n",
    "                         mean_prediction - 2*std_prediction,\n",
    "                         mean_prediction + 2*std_prediction,\n",
    "                         alpha=0.3, color='blue',\n",
    "                         label='95% Confidence Interval')\n",
    "        \n",
    "        high_uncertainty = np.where(std_prediction > np.mean(std_prediction) + np.std(std_prediction))[0]\n",
    "        if len(high_uncertainty) > 0:\n",
    "            plt.scatter(high_uncertainty+1, \n",
    "                       mean_prediction[high_uncertainty],\n",
    "                       color='red', s=50, alpha=0.7, \n",
    "                       label='High Uncertainty Regions')\n",
    "        \n",
    "        plt.title(f\"Prediction Uncertainty Analysis for Sample {i}\")\n",
    "        plt.xlabel(\"Position\")\n",
    "        plt.ylabel(\"Layer Depth (Z)\")\n",
    "        plt.grid(True, alpha=0.3)\n",
    "        if idx == 0:\n",
    "            plt.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    try:\n",
    "        wandb.log({\"uncertainty_analysis\": wandb.Image(plt)})\n",
    "    except:\n",
    "        pass\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_sweep_config():\n",
    "    sweep_config = {\n",
    "        'method': 'bayes', \n",
    "        'metric': {\n",
    "            'name': 'val_nll_score',\n",
    "            'goal': 'minimize'\n",
    "        },\n",
    "        'parameters': {\n",
    "            'model_type': {\n",
    "                'values': ['HybridCNNLSTM']\n",
    "            },\n",
    "            'learning_rate': {\n",
    "                'distribution': 'log_uniform_values', \n",
    "                'min': 3e-4,\n",
    "                'max': 1e-2\n",
    "            },\n",
    "            'batch_size': {\n",
    "                'values': [128]\n",
    "            },\n",
    "            'dropout': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0.1,\n",
    "                'max': 0.3 \n",
    "            },\n",
    "            'hidden_size': {\n",
    "                'values': [128]\n",
    "            },\n",
    "            'num_layers': {\n",
    "                'values': [2, 3, 4]\n",
    "            },\n",
    "            'weight_decay': {\n",
    "                'values': [1e-5, 1e-4]\n",
    "            },\n",
    "            'optimizer': {\n",
    "                'values': ['adamw', 'adamw', 'sgd']\n",
    "            },\n",
    "            'scheduler': {\n",
    "                'values': ['cosine', 'reduce', 'onecycle']\n",
    "            },\n",
    "            'diversity_factor': {\n",
    "                'distribution': 'uniform',\n",
    "                'min': 0.8,\n",
    "                'max': 1.5\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    sweep_config['parameters']['d_model'] = {\n",
    "        'values': [128]\n",
    "    }\n",
    "    sweep_config['parameters']['nhead'] = {\n",
    "        'values': [4, 8, 12]\n",
    "    }\n",
    "    sweep_config['parameters']['dim_feedforward'] = {\n",
    "        'values': [128, 256]\n",
    "    }\n",
    "    \n",
    "    sweep_config['parameters']['kernel_size'] = {\n",
    "        'values': [3, 5]\n",
    "    }\n",
    "    \n",
    "    return sweep_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sweep_agent():\n",
    "    global X_features, y, X_features_test\n",
    "    \n",
    "    # Инициализируем wandb run и сохраняем объект run\n",
    "    run = wandb.init(reinit=True)\n",
    "    \n",
    "    config = wandb.config\n",
    "    model_type = config['model_type']\n",
    "    \n",
    "    default_config = get_default_config(model_type)\n",
    "    \n",
    "    experiment_config = dict(config)\n",
    "    \n",
    "    for key, value in default_config.items():\n",
    "        if key not in experiment_config:\n",
    "            experiment_config[key] = value\n",
    "    \n",
    "    transformer_params = ['d_model', 'nhead', 'dim_feedforward']\n",
    "    tcn_params = ['kernel_size']\n",
    "    \n",
    "    if model_type != 'Transformer':\n",
    "        for param in transformer_params:\n",
    "            if param in experiment_config: \n",
    "                del experiment_config[param]\n",
    "    \n",
    "    if model_type != 'TCN':\n",
    "        for param in tcn_params:\n",
    "            if param in experiment_config:\n",
    "                del experiment_config[param]\n",
    "    \n",
    "    print(f\"Running experiment with config: {experiment_config}\")\n",
    "    \n",
    "    # Передаем объект run в run_experiment\n",
    "    val_nll_score = run_experiment(experiment_config, autofinish=False, wandb_run=run)\n",
    "    \n",
    "    # Логируем с использованием того же объекта run\n",
    "    run.log({'val_nll_score': val_nll_score, 'final_val_nll_score': val_nll_score})\n",
    "    \n",
    "    # wandb.finish() не нужен, так как агент сам позаботится о завершении\n",
    "    \n",
    "    return val_nll_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_sweep(count=10):\n",
    "    # Инициализируем начальный run для создания sweep\n",
    "    init_wandb(project_name=\"geology-forecast-challenge-sweep-gpu-bayes-30-server-HybridCNNLSTM\")\n",
    "    \n",
    "    sweep_config = create_sweep_config()\n",
    "    sweep_id = wandb.sweep(sweep_config, project=\"geology-forecast-challenge-sweep-gpu-bayes-30-server-HybridCNNLSTM\")\n",
    "    \n",
    "    # Завершаем начальный run перед запуском агента\n",
    "    wandb.finish()\n",
    "    \n",
    "    # Запускаем агента\n",
    "    wandb.agent(sweep_id, function=sweep_agent, count=count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_single_model(model_type='HybridCNNLSTM', custom_params=None):\n",
    "    config = get_default_config(model_type)\n",
    "    \n",
    "    if custom_params:\n",
    "        config.update(custom_params)\n",
    "    \n",
    "    val_nll_score = run_experiment(config)\n",
    "    \n",
    "    return val_nll_score, config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nscore, config = run_single_model(\\'HybridCNNLSTM\\', {\\n    \\'hidden_size\\': 512, \\n    \\'num_layers\\': 2,\\n    \\'dropout\\': 0.3,\\n    \\'learning_rate\\': 3e-4,\\n    \\'epochs\\': 60 \\n})\\n\\nprint(f\"Final validation NLL score: {score:.6f}\")\\n'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "score, config = run_single_model('HybridCNNLSTM', {\n",
    "    'hidden_size': 512, \n",
    "    'num_layers': 2,\n",
    "    'dropout': 0.3,\n",
    "    'learning_rate': 3e-4,\n",
    "    'epochs': 60 \n",
    "})\n",
    "\n",
    "print(f\"Final validation NLL score: {score:.6f}\")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Engineering features for train data...\n",
      "Engineering features for test data...\n",
      "Feature shape: (1510, 397), Target shape: (1510, 3000)\n"
     ]
    }
   ],
   "source": [
    "print(\"Engineering features for train data...\")\n",
    "train_features = engineer_features(train)\n",
    "\n",
    "print(\"Engineering features for test data...\")\n",
    "test_features = engineer_features(test)\n",
    "\n",
    "X_features = train_features.drop('geology_id', axis=1).values\n",
    "y = train[TARGETS].values\n",
    "X_features_test = test_features.drop('geology_id', axis=1).values\n",
    "\n",
    "print(f\"Feature shape: {X_features.shape}, Target shape: {y.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run_sweep(count=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =====================================\n",
    "# ОПТИМИЗИРОВАННОЕ ОБУЧЕНИЕ МОДЕЛИ\n",
    "# =====================================\n",
    "\n",
    "# Импорт дополнительных библиотек\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import torch.cuda.amp as amp\n",
    "from torch.utils.data import ConcatDataset\n",
    "import timeit\n",
    "from torch.optim.lr_scheduler import CosineAnnealingWarmRestarts\n",
    "\n",
    "# Определение оптимального конфига для HybridCNNLSTM\n",
    "def get_optimal_config():\n",
    "    return {\n",
    "        'model_type': 'HybridCNNLSTM',\n",
    "        'cnn_filters': [64, 128, 128, 256],  # Увеличены размеры фильтров\n",
    "        'kernel_size': 5,                    # Увеличенное окно свертки\n",
    "        'hidden_size': 128,                  # Увеличен размер скрытого слоя\n",
    "        'num_layers': 3,                     # Добавлен дополнительный слой LSTM\n",
    "        'dropout': 0.1074051389697947,                      # Оптимальное значение дропаута\n",
    "        'learning_rate': 0.001588625112561725,               # Будет адаптироваться с Ranger\n",
    "        'weight_decay': 0.00001,                # Оптимизированное значение регуляризации\n",
    "        'batch_size': 128,                    # Уменьшен размер батча для лучшей сходимости\n",
    "        'epochs': 100,                       # Увеличенное число эпох для K-fold\n",
    "        'seed': SEED,\n",
    "        'feature_engineering': 'advanced',\n",
    "        'optimizer': 'ranger',               # Используем Ranger вместо AdamW\n",
    "        'scheduler': 'cosinewr',             # Cosine annealing with warm restarts\n",
    "        'early_stopping_patience': 15,       # Увеличенное терпение для early stopping\n",
    "        'use_amp': True,                     # Использование mixed precision\n",
    "        'use_swa': True,                     # Stochastic Weight Averaging\n",
    "        'swa_start': 50,                     # Начать SWA с эпохи 50\n",
    "        'mixup_alpha': 0.2,                  # Параметр для Mixup аугментации\n",
    "    }\n",
    "\n",
    "# Реализация оптимизатора Ranger (RAdam + LookAhead)\n",
    "class Ranger(torch.optim.Optimizer):\n",
    "    def __init__(self, params, lr=1e-3, alpha=0.5, k=6, betas=(0.95, 0.999), eps=1e-8, weight_decay=0):\n",
    "        # RAdam params\n",
    "        defaults = dict(lr=lr, alpha=alpha, k=k, betas=betas, eps=eps, weight_decay=weight_decay)\n",
    "        self.buffer = [[None, None, None] for ind in range(10)]\n",
    "        \n",
    "        super(Ranger, self).__init__(params, defaults)\n",
    "        \n",
    "        # Lookаhead параметры\n",
    "        for group in self.param_groups:\n",
    "            group['step_counter'] = 0\n",
    "        \n",
    "        # Создаем копию параметров для LookAhead\n",
    "        self.alpha = alpha\n",
    "        self.k = k\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.requires_grad:\n",
    "                    p.data = p.data.clone()\n",
    "                    state = self.state[p]\n",
    "                    state['slow_params'] = torch.zeros_like(p.data)\n",
    "                    state['slow_params'].copy_(p.data)\n",
    "    \n",
    "    def step(self, closure=None):\n",
    "        loss = None\n",
    "        if closure is not None:\n",
    "            loss = closure()\n",
    "        \n",
    "        for group in self.param_groups:\n",
    "            for p in group['params']:\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                \n",
    "                grad = p.grad.data.float()\n",
    "                if grad.is_sparse:\n",
    "                    raise RuntimeError('Ranger не поддерживает разреженные градиенты')\n",
    "                \n",
    "                p_data_fp32 = p.data.float()\n",
    "                \n",
    "                state = self.state[p]\n",
    "                \n",
    "                if len(state) == 1:\n",
    "                    state['step'] = 0\n",
    "                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n",
    "                else:\n",
    "                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n",
    "                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n",
    "                \n",
    "                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n",
    "                beta1, beta2 = group['betas']\n",
    "                \n",
    "                # RAdam\n",
    "                state['step'] += 1\n",
    "                \n",
    "                # Decay экспоненциально взвешенных скользящих средних\n",
    "                exp_avg.mul_(beta1).add_(grad, alpha=1 - beta1)\n",
    "                exp_avg_sq.mul_(beta2).addcmul_(grad, grad, value=1 - beta2)\n",
    "                \n",
    "                # Применяем корректировку смещения\n",
    "                bias_correction1 = 1 - beta1 ** state['step']\n",
    "                bias_correction2 = 1 - beta2 ** state['step']\n",
    "                step_size = group['lr'] * math.sqrt(bias_correction2) / bias_correction1\n",
    "                \n",
    "                # RAdam часть\n",
    "                buffered = self.buffer[int(state['step'] % 10)]\n",
    "                if state['step'] == buffered[0]:\n",
    "                    N_sma, step_size = buffered[1], buffered[2]\n",
    "                else:\n",
    "                    buffered[0] = state['step']\n",
    "                    beta2_t = beta2 ** state['step']\n",
    "                    N_sma_max = 2 / (1 - beta2) - 1\n",
    "                    N_sma = N_sma_max - 2 * state['step'] * beta2_t / (1 - beta2_t)\n",
    "                    buffered[1] = N_sma\n",
    "                    \n",
    "                    if N_sma >= 5:\n",
    "                        step_size = group['lr'] * math.sqrt((1 - beta2_t) * (N_sma - 4) / (N_sma_max - 4) * (N_sma - 2) / N_sma * N_sma_max / (N_sma_max - 2)) / (1 - beta1 ** state['step'])\n",
    "                    else:\n",
    "                        step_size = group['lr'] / (1 - beta1 ** state['step'])\n",
    "                    buffered[2] = step_size\n",
    "                \n",
    "                if group['weight_decay'] != 0:\n",
    "                    p_data_fp32.add_(p_data_fp32, alpha=-group['weight_decay'] * group['lr'])\n",
    "                \n",
    "                if N_sma >= 5:\n",
    "                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n",
    "                    p_data_fp32.addcdiv_(exp_avg, denom, value=-step_size)\n",
    "                else:\n",
    "                    p_data_fp32.add_(exp_avg, alpha=-step_size)\n",
    "                \n",
    "                p.data.copy_(p_data_fp32)\n",
    "                \n",
    "                # LookAhead часть\n",
    "                group['step_counter'] += 1\n",
    "                if group['step_counter'] % self.k == 0:\n",
    "                    slow_params = state['slow_params']\n",
    "                    slow_params.add_(p.data - slow_params, alpha=self.alpha)\n",
    "                    p.data.copy_(slow_params)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "# Реализация Mixup аугментации\n",
    "def mixup_data(x, y, alpha=0.2):\n",
    "    \"\"\"Создает новые смешанные образцы.\"\"\"\n",
    "    if alpha > 0:\n",
    "        lam = np.random.beta(alpha, alpha)\n",
    "    else:\n",
    "        lam = 1\n",
    "\n",
    "    batch_size = x.size()[0]\n",
    "    index = torch.randperm(batch_size).to(x.device)\n",
    "\n",
    "    mixed_x = lam * x + (1 - lam) * x[index]\n",
    "    y_a, y_b = y, y[index]\n",
    "    return mixed_x, y_a, y_b, lam\n",
    "\n",
    "def mixup_criterion(criterion, pred, y_a, y_b, lam):\n",
    "    \"\"\"Вычисляет потери для смешанных образцов.\"\"\"\n",
    "    return lam * criterion(pred, y_a) + (1 - lam) * criterion(pred, y_b)\n",
    "\n",
    "# Модифицированная функция обучения с применением Mixup и AMP\n",
    "def train_model_with_advanced_techniques(model, train_loader, optimizer, device, scaler, config, epoch=0, total_epochs=30):\n",
    "    model.train()\n",
    "    train_losses = []\n",
    "    \n",
    "    pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{total_epochs}\")\n",
    "    \n",
    "    for data, target in pbar:\n",
    "        data, target = data.to(device, dtype=torch.float32), target.to(device, dtype=torch.float32)\n",
    "        \n",
    "        # Применение Mixup аугментации\n",
    "        if config.get('mixup_alpha', 0) > 0:\n",
    "            data, targets_a, targets_b, lam = mixup_data(data, target, config['mixup_alpha'])\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Используем автоматическую mixed precision\n",
    "        if config.get('use_amp', False):\n",
    "            with amp.autocast():\n",
    "                output = model(data)\n",
    "                \n",
    "                # Нормализация выхода и целевых значений\n",
    "                target_mean = target.mean(dim=0)\n",
    "                target_std = target.std(dim=0) + 1e-6\n",
    "                \n",
    "                normalized_output = (output - target_mean) / target_std\n",
    "                \n",
    "                if config.get('mixup_alpha', 0) > 0:\n",
    "                    normalized_target_a = (targets_a - target_mean) / target_std\n",
    "                    normalized_target_b = (targets_b - target_mean) / target_std\n",
    "                    loss = mixup_criterion(F.mse_loss, normalized_output, normalized_target_a, normalized_target_b, lam)\n",
    "                else:\n",
    "                    normalized_target = (target - target_mean) / target_std\n",
    "                    loss = F.mse_loss(normalized_output, normalized_target)\n",
    "                \n",
    "                # Добавляем штраф за гладкость прогноза\n",
    "                if output.shape[1] > 1:\n",
    "                    smoothness_penalty = torch.mean(torch.abs(output[:, 1:] - output[:, :-1]))\n",
    "                    loss += 0.005 * smoothness_penalty\n",
    "                \n",
    "                # Добавляем штраф за неконсистентность наклона\n",
    "                if output.shape[1] > 2:\n",
    "                    slopes = output[:, 1:] - output[:, :-1]\n",
    "                    slope_changes = slopes[:, 1:] - slopes[:, :-1]\n",
    "                    curvature_penalty = torch.mean(torch.abs(slope_changes))\n",
    "                    loss += 0.001 * curvature_penalty\n",
    "            \n",
    "            # Scale loss и обратное распространение\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.unscale_(optimizer)\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "        else:\n",
    "            # Стандартное обучение без AMP\n",
    "            output = model(data)\n",
    "            \n",
    "            target_mean = target.mean(dim=0)\n",
    "            target_std = target.std(dim=0) + 1e-6\n",
    "            \n",
    "            normalized_output = (output - target_mean) / target_std\n",
    "            \n",
    "            if config.get('mixup_alpha', 0) > 0:\n",
    "                normalized_target_a = (targets_a - target_mean) / target_std\n",
    "                normalized_target_b = (targets_b - target_mean) / target_std\n",
    "                loss = mixup_criterion(F.mse_loss, normalized_output, normalized_target_a, normalized_target_b, lam)\n",
    "            else:\n",
    "                normalized_target = (target - target_mean) / target_std\n",
    "                loss = F.mse_loss(normalized_output, normalized_target)\n",
    "            \n",
    "            if output.shape[1] > 1:\n",
    "                smoothness_penalty = torch.mean(torch.abs(output[:, 1:] - output[:, :-1]))\n",
    "                loss += 0.005 * smoothness_penalty\n",
    "                \n",
    "            if output.shape[1] > 2:\n",
    "                slopes = output[:, 1:] - output[:, :-1]\n",
    "                slope_changes = slopes[:, 1:] - slopes[:, :-1]\n",
    "                curvature_penalty = torch.mean(torch.abs(slope_changes))\n",
    "                loss += 0.001 * curvature_penalty\n",
    "            \n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "        \n",
    "        train_losses.append(loss.item())\n",
    "        pbar.set_postfix({'loss': f\"{loss.item():.6f}\"})\n",
    "    \n",
    "    return np.mean(train_losses)\n",
    "\n",
    "# Реализация Stochastic Weight Averaging\n",
    "class SWA:\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.n_averaged = 0\n",
    "        self.swa_model = copy.deepcopy(model)\n",
    "        \n",
    "        # Устанавливаем веса SWA модели в 0\n",
    "        for param in self.swa_model.parameters():\n",
    "            param.data.zero_()\n",
    "    \n",
    "    def update(self, model):\n",
    "        self.n_averaged += 1\n",
    "        for swa_param, param in zip(self.swa_model.parameters(), model.parameters()):\n",
    "            swa_param.data.mul_(1.0 - 1.0 / self.n_averaged)\n",
    "            swa_param.data.add_(param.data / self.n_averaged)\n",
    "    \n",
    "    def get_model(self):\n",
    "        return self.swa_model\n",
    "\n",
    "# Усовершенствованная функция обучения k-fold\n",
    "def train_kfold_model(X_features, y, X_features_test, n_folds=5):\n",
    "    print(f\"\\n{'='*50}\\nTraining {n_folds}-fold HybridCNNLSTM\\n{'='*50}\")\n",
    "    \n",
    "    config = get_optimal_config()\n",
    "    print(f\"Config: {config}\")\n",
    "    \n",
    "    # Инициализируем wandb\n",
    "    wandb_run = init_wandb(project_name=\"geology-forecast-challenge-improved\", config=config)\n",
    "    \n",
    "    # Подготовка для K-fold\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=config['seed'])\n",
    "    fold_preds_val = np.zeros((X_features.shape[0], y.shape[1]))\n",
    "    fold_preds_test = np.zeros((X_features_test.shape[0], y.shape[1], n_folds))\n",
    "    oof_scores = []\n",
    "    fold_models = []\n",
    "    \n",
    "    # Главный цикл K-fold\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_features)):\n",
    "        print(f\"\\n{'='*30}\\nFold {fold+1}/{n_folds}\\n{'='*30}\")\n",
    "        \n",
    "        # Разделение данных\n",
    "        X_train_fold, X_val_fold = X_features[train_idx], X_features[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "        \n",
    "        # Создание датасетов и загрузчиков\n",
    "        train_dataset = GeologyDataset(X_train_fold, y_train_fold, scale_features=True)\n",
    "        val_dataset = GeologyDataset(X_val_fold, y_val_fold, scale_features=True)\n",
    "        test_dataset = GeologyDataset(X_features_test, is_test=True, scale_features=True)\n",
    "        \n",
    "        train_loader = DataLoader(\n",
    "            train_dataset, \n",
    "            batch_size=config['batch_size'], \n",
    "            shuffle=True,\n",
    "            pin_memory=True, \n",
    "            num_workers=0,\n",
    "            drop_last=True\n",
    "        )\n",
    "        \n",
    "        val_loader = DataLoader(\n",
    "            val_dataset, \n",
    "            batch_size=config['batch_size'] * 2, \n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        test_loader = DataLoader(\n",
    "            test_dataset, \n",
    "            batch_size=config['batch_size'] * 2, \n",
    "            shuffle=False,\n",
    "            pin_memory=True,\n",
    "            num_workers=0\n",
    "        )\n",
    "        \n",
    "        # Создание модели\n",
    "        model = create_model(config, device)\n",
    "        \n",
    "        # Инициализация оптимизатора\n",
    "        if config.get('optimizer', 'ranger').lower() == 'ranger':\n",
    "            optimizer = Ranger(\n",
    "                model.parameters(),\n",
    "                lr=config['learning_rate'],\n",
    "                weight_decay=config['weight_decay']\n",
    "            )\n",
    "        elif config.get('optimizer', 'ranger').lower() == 'adamw':\n",
    "            optimizer = optim.AdamW(\n",
    "                model.parameters(),\n",
    "                lr=config['learning_rate'],\n",
    "                weight_decay=config['weight_decay'],\n",
    "                eps=1e-8\n",
    "            )\n",
    "        else:\n",
    "            optimizer = optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=config['learning_rate'],\n",
    "                weight_decay=config['weight_decay'],\n",
    "                eps=1e-8\n",
    "            )\n",
    "        \n",
    "        # Инициализация планировщика\n",
    "        if config.get('scheduler', 'cosinewr').lower() == 'cosinewr':\n",
    "            scheduler = CosineAnnealingWarmRestarts(\n",
    "                optimizer,\n",
    "                T_0=5, \n",
    "                T_mult=2,\n",
    "                eta_min=config['learning_rate'] / 100\n",
    "            )\n",
    "        elif config.get('scheduler', 'cosinewr').lower() == 'onecycle':\n",
    "            steps_per_epoch = len(train_loader)\n",
    "            scheduler = optim.lr_scheduler.OneCycleLR(\n",
    "                optimizer,\n",
    "                max_lr=config['learning_rate'],\n",
    "                epochs=config['epochs'],\n",
    "                steps_per_epoch=steps_per_epoch,\n",
    "                pct_start=0.3,\n",
    "                div_factor=25,\n",
    "                final_div_factor=1000,\n",
    "            )\n",
    "        else:\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(\n",
    "                optimizer, \n",
    "                T_max=config['epochs'],\n",
    "                eta_min=config['learning_rate'] / 100\n",
    "            )\n",
    "        \n",
    "        # Инициализация SWA\n",
    "        swa_model = None\n",
    "        if config.get('use_swa', False):\n",
    "            swa_model = SWA(model)\n",
    "        \n",
    "        # Инициализация scaler для AMP\n",
    "        scaler = amp.GradScaler() if config.get('use_amp', False) else None\n",
    "        \n",
    "        # Параметры для раннего останова\n",
    "        best_val_loss = float('inf')\n",
    "        patience = config.get('early_stopping_patience', 15)\n",
    "        counter = 0\n",
    "        min_delta = 1e-4\n",
    "        best_val_preds = None\n",
    "        best_epoch = 0\n",
    "        \n",
    "        # Обучение модели\n",
    "        for epoch in range(config['epochs']):\n",
    "            start_time = timeit.default_timer()\n",
    "            \n",
    "            # Обучение эпохи\n",
    "            train_loss = train_model_with_advanced_techniques(\n",
    "                model, train_loader, optimizer, device, scaler, config, epoch, config['epochs']\n",
    "            )\n",
    "            \n",
    "            # Обновление планировщика\n",
    "            if config.get('scheduler', 'cosinewr').lower() != 'reduce':\n",
    "                scheduler.step()\n",
    "            \n",
    "            # Валидация\n",
    "            val_loss, val_preds, val_targets = validate_model(model, val_loader, device)\n",
    "            \n",
    "            # Обновление лучших весов\n",
    "            if val_loss < best_val_loss:\n",
    "                best_val_loss = val_loss\n",
    "                best_val_preds = val_preds\n",
    "                best_epoch = epoch\n",
    "                \n",
    "                model_path = f\"model_fold_{fold}.pt\"\n",
    "                torch.save(model.state_dict(), model_path)\n",
    "                \n",
    "                if wandb_run:\n",
    "                    wandb_run.save(model_path)\n",
    "                    \n",
    "                print(f\"✅ Saved new best model with validation loss: {val_loss:.6f}\")\n",
    "                counter = 0\n",
    "            else:\n",
    "                counter += 1\n",
    "                \n",
    "            # Обновление SWA\n",
    "            if config.get('use_swa', False) and epoch >= config.get('swa_start', 50):\n",
    "                swa_model.update(model)\n",
    "            \n",
    "            # Вычисление времени эпохи\n",
    "            elapsed = timeit.default_timer() - start_time\n",
    "            \n",
    "            # Логирование метрик\n",
    "            if wandb_run:\n",
    "                wandb_run.log({\n",
    "                    f\"fold_{fold}_epoch\": epoch + 1,\n",
    "                    f\"fold_{fold}_train_loss\": train_loss,\n",
    "                    f\"fold_{fold}_val_loss\": val_loss,\n",
    "                    f\"fold_{fold}_learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                    f\"fold_{fold}_epoch_time\": elapsed\n",
    "                })\n",
    "            \n",
    "            print(f\"Epoch {epoch+1}/{config['epochs']} - Train Loss: {train_loss:.6f}, Val Loss: {val_loss:.6f}, LR: {optimizer.param_groups[0]['lr']:.8f}, Time: {elapsed:.2f}s\")\n",
    "            \n",
    "            # Проверка раннего останова\n",
    "            if counter >= patience:\n",
    "                print(f\"Early stopping at epoch {epoch+1}, best val_loss: {best_val_loss:.6f}\")\n",
    "                break\n",
    "        \n",
    "        # Загрузка лучшей модели для предсказаний\n",
    "        print(f\"Loading best model from epoch {best_epoch+1}\")\n",
    "        model.load_state_dict(torch.load(f\"model_fold_{fold}.pt\"))\n",
    "        \n",
    "        # Если использовали SWA, проверяем производительность SWA модели\n",
    "        if config.get('use_swa', False) and best_epoch >= config.get('swa_start', 50):\n",
    "            swa_model_copy = swa_model.get_model()\n",
    "            swa_val_loss, swa_val_preds, _ = validate_model(swa_model_copy, val_loader, device)\n",
    "            \n",
    "            print(f\"SWA Model Val Loss: {swa_val_loss:.6f} vs Best Model Val Loss: {best_val_loss:.6f}\")\n",
    "            \n",
    "            # Если SWA модель лучше, используем её\n",
    "            if swa_val_loss < best_val_loss:\n",
    "                print(\"Using SWA model for predictions\")\n",
    "                model = swa_model_copy\n",
    "                best_val_preds = swa_val_preds\n",
    "                best_val_loss = swa_val_loss\n",
    "        \n",
    "        # Сохраняем предсказания валидации\n",
    "        fold_preds_val[val_idx] = best_val_preds\n",
    "        \n",
    "        # Создаем предсказания для тестовых данных\n",
    "        model.eval()\n",
    "        test_preds = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for data in tqdm(test_loader, desc=\"Predicting test data\"):\n",
    "                if isinstance(data, list):\n",
    "                    data = data[0]\n",
    "                data = data.to(device, dtype=torch.float32)\n",
    "                output = model(data)\n",
    "                test_preds.append(output.cpu().numpy())\n",
    "        \n",
    "        test_fold_preds = np.concatenate(test_preds)\n",
    "        fold_preds_test[:, :, fold] = test_fold_preds\n",
    "        \n",
    "        # Вычисление NLL метрики для фолда\n",
    "        val_preds_df = pd.DataFrame(\n",
    "            data=best_val_preds,\n",
    "            columns=TARGETS,\n",
    "        )\n",
    "        val_preds_df['geology_id'] = train.iloc[val_idx]['geology_id'].values\n",
    "        \n",
    "        val_solution_df = pd.DataFrame(\n",
    "            data=y_val_fold,\n",
    "            columns=TARGETS,\n",
    "        )\n",
    "        val_solution_df['geology_id'] = train.iloc[val_idx]['geology_id'].values\n",
    "        \n",
    "        fold_nll_score = compute_nll_score(val_solution_df, val_preds_df)\n",
    "        oof_scores.append(fold_nll_score)\n",
    "        \n",
    "        print(f\"Fold {fold+1} NLL Score: {fold_nll_score:.6f}\")\n",
    "        \n",
    "        # Сохраняем модель для ансамбля\n",
    "        fold_models.append(model)\n",
    "        \n",
    "        # Очистка после каждого фолда\n",
    "        torch.cuda.empty_cache()\n",
    "        import gc\n",
    "        gc.collect()\n",
    "    \n",
    "    # Вычисление среднего OOF скора\n",
    "    mean_oof_score = np.mean(oof_scores)\n",
    "    print(f\"\\nMean OOF NLL Score: {mean_oof_score:.6f}\")\n",
    "    \n",
    "    if wandb_run:\n",
    "        wandb_run.log({\"mean_oof_nll_score\": mean_oof_score})\n",
    "        for i, score in enumerate(oof_scores):\n",
    "            wandb_run.log({f\"fold_{i}_nll_score\": score})\n",
    "    \n",
    "    # Усреднение предсказаний по всем фолдам\n",
    "    test_predictions = np.mean(fold_preds_test, axis=2)\n",
    "    \n",
    "    # Создание сабмишна\n",
    "    submission = create_optimized_submission(test_predictions, mean_oof_score)\n",
    "    \n",
    "    # Визуализация результатов\n",
    "    visualize_predictions(submission, 0, 3)\n",
    "    analyze_geological_patterns(submission)\n",
    "    calculate_prediction_uncertainty(submission)\n",
    "    \n",
    "    if wandb_run:\n",
    "        wandb_run.log({\"final_submission\": wandb.Table(dataframe=submission.head())})\n",
    "        wandb_run.finish()\n",
    "    \n",
    "    return submission, mean_oof_score, fold_preds_val, fold_models\n",
    "\n",
    "# Оптимизированное создание разнообразных реализаций\n",
    "def generate_optimized_realizations(base_predictions, num_samples=10):\n",
    "    num_rows, num_cols = base_predictions.shape\n",
    "    realizations = np.zeros((num_samples, num_rows, num_cols))\n",
    "    \n",
    "    # Первая реализация - базовая\n",
    "    realizations[0] = base_predictions\n",
    "    \n",
    "    # Параметры шума, зависящие от позиции\n",
    "    position_factors = np.linspace(0.1, 1.5, num_cols) ** 1.5\n",
    "    \n",
    "    # Создаем оптимизированный шум для каждой реализации\n",
    "    for i in range(1, num_samples):\n",
    "        realization = base_predictions.copy()\n",
    "        \n",
    "        # Добавляем разную степень случайности к разным реализациям\n",
    "        diversity_factor = 1.3818770228728243\n",
    "        \n",
    "        for j in range(num_rows):\n",
    "            # Создаем базовый шум\n",
    "            noise = np.random.normal(0, diversity_factor, num_cols)\n",
    "            \n",
    "            # Сглаживаем шум с разными уровнями сглаживания\n",
    "            smoothed_noise = gaussian_filter1d(noise, sigma=3.0 + 2.0 * (i % 3))\n",
    "            \n",
    "            # Масштабируем шум в зависимости от позиции\n",
    "            scaled_noise = smoothed_noise * position_factors\n",
    "            \n",
    "            # Добавляем периодичность к некоторым реализациям\n",
    "            if i % 3 == 0:\n",
    "                periodic_component = np.sin(np.linspace(0, i * np.pi, num_cols)) * diversity_factor * 0.5\n",
    "                scaled_noise += periodic_component\n",
    "            \n",
    "            # Применяем шум\n",
    "            realization[j] += scaled_noise\n",
    "            \n",
    "            # Обеспечиваем геологическую реалистичность - ограничиваем резкие изменения\n",
    "            for k in range(1, num_cols):\n",
    "                # Ограничение максимального изменения, возрастающее с расстоянием\n",
    "                max_change = 1.5 * (1 + k/num_cols)\n",
    "                diff = realization[j, k] - realization[j, k-1]\n",
    "                if abs(diff) > max_change:\n",
    "                    realization[j, k] = realization[j, k-1] + np.sign(diff) * max_change\n",
    "        \n",
    "        realizations[i] = realization\n",
    "    \n",
    "    return realizations\n",
    "\n",
    "# Оптимизированное создание сабмишна\n",
    "def create_optimized_submission(base_predictions, val_nll_score):\n",
    "    submission = sub.copy()\n",
    "    \n",
    "    # Заполняем основные предсказания\n",
    "    for i in range(300):\n",
    "        col_name = str(i+1)\n",
    "        submission[col_name] = base_predictions[:, i]\n",
    "    \n",
    "    # Создаем разнообразные реализации\n",
    "    realizations = generate_optimized_realizations(base_predictions, num_samples=10)\n",
    "    \n",
    "    # Заполняем колонки реализаций\n",
    "    for r_idx in range(1, 10): \n",
    "        for i in range(300):\n",
    "            col_name = f\"r_{r_idx}_pos_{i+1}\"\n",
    "            submission[col_name] = realizations[r_idx][:, i]\n",
    "    \n",
    "    # Сохраняем сабмишн\n",
    "    submission_file = f\"submission_HybridCNNLSTM_kfold_{val_nll_score:.6f}.csv\"\n",
    "    submission.to_csv(submission_file, index=False)\n",
    "    print(f\"\\nSubmission file saved: {submission_file}\")\n",
    "    \n",
    "    # Проверяем корректность формата\n",
    "    expected_cols = sub.columns.tolist()\n",
    "    actual_cols = submission.columns.tolist()\n",
    "    \n",
    "    if set(expected_cols) != set(actual_cols):\n",
    "        print(\"WARNING: Submission columns don't match expected format!\")\n",
    "        missing = set(expected_cols) - set(actual_cols)\n",
    "        extra = set(actual_cols) - set(expected_cols)\n",
    "        if missing:\n",
    "            print(f\"Missing columns: {missing}\")\n",
    "        if extra:\n",
    "            print(f\"Extra columns: {extra}\")\n",
    "    else:\n",
    "        print(\"Submission format validated successfully!\")\n",
    "    \n",
    "    return submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('spawn', force=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Training 5-fold HybridCNNLSTM\n",
      "==================================================\n",
      "Config: {'model_type': 'LSTM', 'cnn_filters': [64, 128, 128, 256], 'kernel_size': 5, 'hidden_size': 128, 'num_layers': 3, 'dropout': 0.1074051389697947, 'learning_rate': 0.001588625112561725, 'weight_decay': 1e-05, 'batch_size': 128, 'epochs': 100, 'seed': 42, 'feature_engineering': 'advanced', 'optimizer': 'ranger', 'scheduler': 'cosinewr', 'early_stopping_patience': 15, 'use_amp': True, 'use_swa': True, 'swa_start': 50, 'mixup_alpha': 0.2}\n",
      "\n",
      "==============================\n",
      "Fold 1/5\n",
      "==============================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f97d1308fad84efd8807b0dafe0461b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fc97cc4a0e84c1b9c6636c6c15ed25e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 1.150023\n",
      "Epoch 1/100 - Train Loss: 1.180127, Val Loss: 1.150023, LR: 0.00143844, Time: 0.43s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fe9376f807e4537ab61269cfa697af0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fe342701ff1473996817013be7beaca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 1.074035\n",
      "Epoch 2/100 - Train Loss: 1.095573, Val Loss: 1.074035, LR: 0.00104526, Time: 0.21s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9c29c1df984478f95f01711a171ff0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5dea86acc4224697b95dbea504715916",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 1.062486\n",
      "Epoch 3/100 - Train Loss: 1.046933, Val Loss: 1.062486, LR: 0.00055925, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00d7e21fa14b48bc8e12a80a75bc9f38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "704f6c7cad374fe899db0896487cc46b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 1.054982\n",
      "Epoch 4/100 - Train Loss: 1.034230, Val Loss: 1.054982, LR: 0.00016607, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af0e5a1302543658feee76993f587f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a957611a761e45969657ebf306125622",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 1.052386\n",
      "Epoch 5/100 - Train Loss: 1.032061, Val Loss: 1.052386, LR: 0.00158863, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f9b830be61842f2853aeac0905c161f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f8dae69f14547a9ad37ddbf0dcb6c02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 1.047721\n",
      "Epoch 6/100 - Train Loss: 1.025307, Val Loss: 1.047721, LR: 0.00155014, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f9fbff5871c4508a7a58ffddb72db72",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4d1920f2ca8440394f1540caac6ca6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 1.031776\n",
      "Epoch 7/100 - Train Loss: 1.021996, Val Loss: 1.031776, LR: 0.00143844, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aea5d788632f469f87fe8a381bd5def6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4450c67bf9fc4c24af571ad6aa8770c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 1.009884\n",
      "Epoch 8/100 - Train Loss: 1.002740, Val Loss: 1.009884, LR: 0.00126447, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40ff8abbab7f475d93fa496b80b5e836",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88106ab2b6f48cdbac6faffdfa47d56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.968736\n",
      "Epoch 9/100 - Train Loss: 0.973686, Val Loss: 0.968736, LR: 0.00104526, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b56df90a6ba4f649ca305d976d3e424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85000a014fb64ba4a63dfed66a7c874e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.916472\n",
      "Epoch 10/100 - Train Loss: 0.949311, Val Loss: 0.916472, LR: 0.00080226, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fcadf94417d469493d9ea0091d557a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10dd7c8f4c37412b8b6dc6ab4caab691",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.871861\n",
      "Epoch 11/100 - Train Loss: 0.906015, Val Loss: 0.871861, LR: 0.00055925, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db1952684e7a471eab85dc716bce533b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c24e1b02943b4ab9a79b23b3544c52af",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.835332\n",
      "Epoch 12/100 - Train Loss: 0.877527, Val Loss: 0.835332, LR: 0.00034004, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "14206f857a1f418fb2faa78ff0ac29c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82ca892b179f4eae8dba74255048516f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.813328\n",
      "Epoch 13/100 - Train Loss: 0.855980, Val Loss: 0.813328, LR: 0.00016607, Time: 0.21s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "042250a4c424487ebc0d7f197c638470",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a5155f9183f42f4ab2792b08ad704c6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.802323\n",
      "Epoch 14/100 - Train Loss: 0.852071, Val Loss: 0.802323, LR: 0.00005437, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6e2898ad5054bb58521bb92b5c2343b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "159a19cbd892466d96442e92b35ee7f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.798511\n",
      "Epoch 15/100 - Train Loss: 0.831454, Val Loss: 0.798511, LR: 0.00158863, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93bfa1f976644f7e87335ee7888a9fe2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5357b573116e4ab69e08d45fdee3db0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.682341\n",
      "Epoch 16/100 - Train Loss: 0.760067, Val Loss: 0.682341, LR: 0.00157894, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f00bb42a011c4df995a7f4c9c4252a71",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7237246f3644250a997b2e1641cdef8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.590923\n",
      "Epoch 17/100 - Train Loss: 0.705493, Val Loss: 0.590923, LR: 0.00155014, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e4fac33a7604d1abb16acc6cf4c609f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "29c3a2ccaad24c4fb55754055c183d10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.513057\n",
      "Epoch 18/100 - Train Loss: 0.686112, Val Loss: 0.513057, LR: 0.00150292, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "837a9f3fe8e6471e900a28369740d8df",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "34836374e0324bc29501a3de704b4edb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.435547\n",
      "Epoch 19/100 - Train Loss: 0.540475, Val Loss: 0.435547, LR: 0.00143844, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "415d42472c3c47bba1bcc7819b287550",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6fb42eacbeb5479f93987122211d4e95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.392738\n",
      "Epoch 20/100 - Train Loss: 0.525653, Val Loss: 0.392738, LR: 0.00135830, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35e9e2015c484941a4f453d3555bb352",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5cd91a45deb4fa28cae019eddec38a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.347056\n",
      "Epoch 21/100 - Train Loss: 0.441865, Val Loss: 0.347056, LR: 0.00126447, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffd8d737859c48d3bf3d3ed86e9ed04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8a932b17c546407d86ab918d005425cb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.304685\n",
      "Epoch 22/100 - Train Loss: 0.443825, Val Loss: 0.304685, LR: 0.00115926, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27c8c76baf9c4987b78315a504b9ef5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e5053daeedb4544a63c6c13dee3da3d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.289234\n",
      "Epoch 23/100 - Train Loss: 0.377627, Val Loss: 0.289234, LR: 0.00104526, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc5a46f11a7c4d50a6661cd201892737",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b79b1bc4e504539802767807f56a9b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.268708\n",
      "Epoch 24/100 - Train Loss: 0.390614, Val Loss: 0.268708, LR: 0.00092527, Time: 0.21s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d3fd9feede14eb4aa258f25aa5cbfe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "13cf1d0dd0b14ac99cd26cfb176b679e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.256774\n",
      "Epoch 25/100 - Train Loss: 0.365791, Val Loss: 0.256774, LR: 0.00080226, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "062be5a3127f480da6113cb7d1f5f0fa",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 26/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "edcbb6620c3c4993aee8a0fb309f9d17",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.240503\n",
      "Epoch 26/100 - Train Loss: 0.320772, Val Loss: 0.240503, LR: 0.00067924, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ee38bb51b7aa4436b4ad3c7d8efbf506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 27/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71f123b83884b5a9fbadf202717cb00",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.235013\n",
      "Epoch 27/100 - Train Loss: 0.335012, Val Loss: 0.235013, LR: 0.00055925, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "beb8e9cdd5214709a7ce795e93c631a3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 28/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89ee6b872dbe4326babd2e52f527ba98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.233199\n",
      "Epoch 28/100 - Train Loss: 0.364776, Val Loss: 0.233199, LR: 0.00044525, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de2202a2c137441cbaef60a35f87cee9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 29/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6cbca93fdcf84cc9b152466cae605d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.224753\n",
      "Epoch 29/100 - Train Loss: 0.324386, Val Loss: 0.224753, LR: 0.00034004, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8247a146fe4e5d924d149d88c50bbd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 30/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eeb7669102a14614b4e1290f9352e99c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100 - Train Loss: 0.382484, Val Loss: 0.226647, LR: 0.00024621, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493e0dc221554e2cad2dcd8fd35cd4de",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 31/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bede53ecd83b4ba4b8066795c22940d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.222793\n",
      "Epoch 31/100 - Train Loss: 0.323997, Val Loss: 0.222793, LR: 0.00016607, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "840cad41e97343d1a7e6ddb1a663639e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 32/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1771f8f6ada0406c893f9df8949eb061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.220525\n",
      "Epoch 32/100 - Train Loss: 0.300276, Val Loss: 0.220525, LR: 0.00010160, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c4e115d5f944428be2b98c5960b0ae6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 33/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a685d5b822b48b6a594e46c3e4cec49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.219237\n",
      "Epoch 33/100 - Train Loss: 0.374160, Val Loss: 0.219237, LR: 0.00005437, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2019b29b692142ff9760af07792bc3be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 34/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42774725d05f495f9446f90d052db066",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100 - Train Loss: 0.382908, Val Loss: 0.219927, LR: 0.00002557, Time: 0.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a903be7208414ba9b7eee05631927b51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 35/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a50397c27454a05b7fd15b9b4be4710",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/100 - Train Loss: 0.355369, Val Loss: 0.219957, LR: 0.00158863, Time: 0.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "72cd1b76fe7641faad6fdc4c36ddad86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 36/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ce0b495f6f047e1bf431917a45f822b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.216451\n",
      "Epoch 36/100 - Train Loss: 0.346091, Val Loss: 0.216451, LR: 0.00158620, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e85e887a6bf74b3cb1ce0000b6a24689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 37/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "904cf1b21dd144b9ab887f7138bf92a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.203342\n",
      "Epoch 37/100 - Train Loss: 0.399538, Val Loss: 0.203342, LR: 0.00157894, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d53ed33bfec4e0d8646fc81cfd5eebb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 38/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874ca633cbe64893a4dcdd9eab00e090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/100 - Train Loss: 0.359113, Val Loss: 0.205732, LR: 0.00156690, Time: 0.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1a65b9a472994f05ba790e289395e0f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 39/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce8509a39f5a4a0fbba5afc7342dae0a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/100 - Train Loss: 0.322984, Val Loss: 0.204094, LR: 0.00155014, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b9e3a62b15d4d44a9dd970508ce4c5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 40/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99c139dd7b64ccf851af5da921c655f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.193306\n",
      "Epoch 40/100 - Train Loss: 0.357368, Val Loss: 0.193306, LR: 0.00152877, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f2b0f1e07c7470ab433481a09852e2f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 41/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de30e612fb9343e280b31aee1c0e8307",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.185716\n",
      "Epoch 41/100 - Train Loss: 0.327007, Val Loss: 0.185716, LR: 0.00150292, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b21c6b1156948758c95e016a3a55676",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 42/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be456a319cd24e6fabdf0b6c3c63532c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/100 - Train Loss: 0.406792, Val Loss: 0.200083, LR: 0.00147275, Time: 0.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9cca6361c1cf4e5593d06e202e66110c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 43/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a47d073e86c4439fa75e587ebfc0205f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/100 - Train Loss: 0.335336, Val Loss: 0.193819, LR: 0.00143844, Time: 0.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e97a270d23945d680d6c36b80f88dd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 44/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3924445cbb70447c8760a34e99e2b0f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/100 - Train Loss: 0.263321, Val Loss: 0.190116, LR: 0.00140022, Time: 0.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cb17ed742e4f45a5a224f37da4d572",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 45/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a15a434a8204a9ab129457749511bb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.185577\n",
      "Epoch 45/100 - Train Loss: 0.277936, Val Loss: 0.185577, LR: 0.00135830, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c9e907978634c1597cc85fb49b2b530",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 46/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e69c7d5a0c604389a7d2936efd834234",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.182991\n",
      "Epoch 46/100 - Train Loss: 0.323251, Val Loss: 0.182991, LR: 0.00131296, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a103fd4ec0bc4ab4a943a5198c85cf7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 47/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5601dbf4141249609e2be1b6c7737cd7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/100 - Train Loss: 0.386490, Val Loss: 0.196708, LR: 0.00126447, Time: 0.17s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "810f798636e4418f89bd319f4c4312e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 48/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cf12b38bd74e46c9b9d8db21c9901776",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.181884\n",
      "Epoch 48/100 - Train Loss: 0.258940, Val Loss: 0.181884, LR: 0.00121313, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f59874c3d8e40d6b49aa0ef13e83cf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 49/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0f780d3b61b64cf7b95330eaaae6edf2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.180256\n",
      "Epoch 49/100 - Train Loss: 0.299658, Val Loss: 0.180256, LR: 0.00115926, Time: 0.19s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c994d41b55e4757ae880154028b85f9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 50/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a64e70c4a1184157bee5a5446ee5721d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/100 - Train Loss: 0.339309, Val Loss: 0.182858, LR: 0.00110319, Time: 0.17s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f7de4930e200462fb4c9b283407755bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 51/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "476d92e359c84248915e5eea7db71fbb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Saved new best model with validation loss: 0.177386\n",
      "Epoch 51/100 - Train Loss: 0.381950, Val Loss: 0.177386, LR: 0.00104526, Time: 0.20s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e4bc15f46154ff4a413c202e8357464",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 52/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1724fb42129544c2bda10bc03b5416da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/100 - Train Loss: 0.223389, Val Loss: 0.178215, LR: 0.00098583, Time: 0.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40cd53b8e69b48aa91c7044a149eae15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 53/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "96834689b7a24b2bbe7e823596689a45",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 53/100 - Train Loss: 0.251596, Val Loss: 0.184537, LR: 0.00092527, Time: 0.18s\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "336702809ffc41f1873345b748dc898b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 54/100:   0%|          | 0/9 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7ea34730382d4905b3db98b02ad67287",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validating:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[44], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtrain_kfold_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_features_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_folds\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[42], line 403\u001b[0m, in \u001b[0;36mtrain_kfold_model\u001b[0;34m(X_features, y, X_features_test, n_folds)\u001b[0m\n\u001b[1;32m    400\u001b[0m     scheduler\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    402\u001b[0m \u001b[38;5;66;03m# Валидация\u001b[39;00m\n\u001b[0;32m--> 403\u001b[0m val_loss, val_preds, val_targets \u001b[38;5;241m=\u001b[39m \u001b[43mvalidate_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    405\u001b[0m \u001b[38;5;66;03m# Обновление лучших весов\u001b[39;00m\n\u001b[1;32m    406\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m val_loss \u001b[38;5;241m<\u001b[39m best_val_loss:\n",
      "Cell \u001b[0;32mIn[18], line 20\u001b[0m, in \u001b[0;36mvalidate_model\u001b[0;34m(model, val_loader, device)\u001b[0m\n\u001b[1;32m     16\u001b[0m normalized_target \u001b[38;5;241m=\u001b[39m (target \u001b[38;5;241m-\u001b[39m target_mean) \u001b[38;5;241m/\u001b[39m target_std\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(normalized_output, normalized_target)\n\u001b[0;32m---> 20\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     21\u001b[0m val_preds\u001b[38;5;241m.\u001b[39mappend(output\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n\u001b[1;32m     22\u001b[0m val_targets\u001b[38;5;241m.\u001b[39mappend(target\u001b[38;5;241m.\u001b[39mcpu()\u001b[38;5;241m.\u001b[39mnumpy())\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# train_kfold_model(X_features, y, X_features_test, n_folds=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
      "Requirement already satisfied: numpy in /opt/venv/lib/python3.10/site-packages (from xgboost) (1.26.4)\n",
      "Requirement already satisfied: nvidia-nccl-cu12 in /opt/venv/lib/python3.10/site-packages (from xgboost) (2.21.5)\n",
      "Requirement already satisfied: scipy in /opt/venv/lib/python3.10/site-packages (from xgboost) (1.13.0)\n",
      "Downloading xgboost-3.0.0-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m30.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: xgboost\n",
      "Successfully installed xgboost-3.0.0\n"
     ]
    }
   ],
   "source": [
    "!pip install xgboost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Импорт необходимых библиотек\n",
    "import copy\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import xgboost as xgb\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import torch  # Для проверки доступности GPU\n",
    "import wandb  # Если используется Wandb для логирования\n",
    "\n",
    "# Константы\n",
    "SEED = 42\n",
    "TARGETS = [f'target_{i}' for i in range(1, 6)]  # Пример названий целевых переменных\n",
    "\n",
    "# Функция создания модели XGBoost\n",
    "def create_xgboost_model(config):\n",
    "    \"\"\"Создание модели XGBoost с MultiOutputRegressor\"\"\"\n",
    "    xgb_params = {\n",
    "        'n_estimators': config.get('n_estimators', 1000),\n",
    "        'max_depth': config.get('max_depth', 8),\n",
    "        'learning_rate': config.get('learning_rate', 0.01),\n",
    "        'subsample': config.get('subsample', 0.8),\n",
    "        'colsample_bytree': config.get('colsample_bytree', 0.8),\n",
    "        'reg_alpha': config.get('reg_alpha', 0.01),\n",
    "        'reg_lambda': config.get('reg_lambda', 1.0),\n",
    "        'min_child_weight': config.get('min_child_weight', 3),\n",
    "        'gamma': config.get('gamma', 0.0),\n",
    "        'random_state': config.get('seed', SEED),\n",
    "        'n_jobs': -1,\n",
    "        'tree_method': 'gpu_hist' if torch.cuda.is_available() else 'hist',\n",
    "        'enable_categorical': False,\n",
    "    }\n",
    "    \n",
    "    base_model = xgb.XGBRegressor(**xgb_params)\n",
    "    return MultiOutputRegressor(base_model)\n",
    "\n",
    "# Функция обучения модели\n",
    "def train_xgboost_model(model, X_train, y_train, X_val=None, y_val=None, config=None):\n",
    "    \"\"\"Обучение модели с поддержкой ранней остановки\"\"\"\n",
    "    # Преобразование данных в numpy arrays\n",
    "    X_train = np.asarray(X_train, dtype=np.float32)\n",
    "    y_train = np.asarray(y_train, dtype=np.float32)\n",
    "    \n",
    "    eval_set = None\n",
    "    if X_val is not None and y_val is not None:\n",
    "        X_val = np.asarray(X_val, dtype=np.float32)\n",
    "        y_val = np.asarray(y_val, dtype=np.float32)\n",
    "        eval_set = [(X_val, y_val)]\n",
    "\n",
    "    # Параметры обучения\n",
    "    fit_params = {\n",
    "        'eval_set': eval_set,\n",
    "        'early_stopping_rounds': config.get('early_stopping_patience', 20) if eval_set else None,\n",
    "        'verbose': False\n",
    "    }\n",
    "\n",
    "    # Обучение модели\n",
    "    model.fit(X_train, y_train, **fit_params)\n",
    "    return model\n",
    "\n",
    "# Функция предсказания\n",
    "def predict_with_xgboost(model, X):\n",
    "    \"\"\"Предсказание для всех целевых переменных\"\"\"\n",
    "    return model.predict(np.asarray(X, dtype=np.float32))\n",
    "\n",
    "# Функция K-fold обучения\n",
    "def train_kfold_xgboost_model(X_features, y, X_features_test, n_folds=5):\n",
    "    \"\"\"Основная функция для кросс-валидации\"\"\"\n",
    "    print(f\"\\n{'='*50}\\nTraining {n_folds}-fold XGBoost\\n{'='*50}\")\n",
    "    \n",
    "    config = get_optimal_xgboost_config()\n",
    "    print(f\"Config: {config}\")\n",
    "    \n",
    "    # Инициализация Wandb\n",
    "    wandb_run = init_wandb(project_name=\"geology-forecast-xgboost\", config=config) if wandb else None\n",
    "    \n",
    "    # Подготовка структур данных\n",
    "    kf = KFold(n_splits=n_folds, shuffle=True, random_state=config['seed'])\n",
    "    fold_preds_val = np.zeros((X_features.shape[0], y.shape[1]))\n",
    "    fold_preds_test = np.zeros((X_features_test.shape[0], y.shape[1], n_folds))\n",
    "    oof_scores = []\n",
    "    models = []\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(X_features)):\n",
    "        print(f\"\\n{'='*30}\\nFold {fold+1}/{n_folds}\\n{'='*30}\")\n",
    "        \n",
    "        # Разделение данных\n",
    "        X_train_fold, X_val_fold = X_features[train_idx], X_features[val_idx]\n",
    "        y_train_fold, y_val_fold = y[train_idx], y[val_idx]\n",
    "\n",
    "        # Масштабирование\n",
    "        scaler = StandardScaler()\n",
    "        X_train_fold = scaler.fit_transform(X_train_fold)\n",
    "        X_val_fold = scaler.transform(X_val_fold)\n",
    "        X_test_scaled = scaler.transform(X_features_test)\n",
    "\n",
    "        # Создание и обучение модели\n",
    "        model = create_xgboost_model(config)\n",
    "        \n",
    "        start_time = timeit.default_timer()\n",
    "        model = train_xgboost_model(\n",
    "            model, \n",
    "            X_train_fold, \n",
    "            y_train_fold,\n",
    "            X_val_fold,\n",
    "            y_val_fold,\n",
    "            config\n",
    "        )\n",
    "        elapsed = timeit.default_timer() - start_time\n",
    "        \n",
    "        print(f\"Training completed in {elapsed:.2f} seconds\")\n",
    "\n",
    "        # Предсказания\n",
    "        val_preds = predict_with_xgboost(model, X_val_fold)\n",
    "        fold_preds_val[val_idx] = val_preds\n",
    "        \n",
    "        test_preds = predict_with_xgboost(model, X_test_scaled)\n",
    "        fold_preds_test[:, :, fold] = test_preds\n",
    "\n",
    "        # Вычисление метрики\n",
    "        fold_nll_score = compute_nll_score(y_val_fold, val_preds)  # Предполагаем существование этой функции\n",
    "        oof_scores.append(fold_nll_score)\n",
    "        print(f\"Fold {fold+1} NLL Score: {fold_nll_score:.6f}\")\n",
    "\n",
    "        # Сохранение модели и логирование\n",
    "        models.append(copy.deepcopy(model))\n",
    "        if wandb_run:\n",
    "            wandb_run.log({\n",
    "                f\"fold_{fold+1}_nll\": fold_nll_score,\n",
    "                f\"fold_{fold+1}_time\": elapsed\n",
    "            })\n",
    "\n",
    "        # Очистка памяти\n",
    "        del model\n",
    "        import gc\n",
    "        gc.collect()\n",
    "\n",
    "    # Финализация результатов\n",
    "    mean_oof_score = np.mean(oof_scores)\n",
    "    print(f\"\\nMean OOF NLL Score: {mean_oof_score:.6f}\")\n",
    "    \n",
    "    # Усреднение предсказаний\n",
    "    test_predictions = np.mean(fold_preds_test, axis=2)\n",
    "    \n",
    "    # Создание сабмишна\n",
    "    submission = create_optimized_submission(test_predictions)  # Предполагаем существование этой функции\n",
    "    \n",
    "    if wandb_run:\n",
    "        wandb_run.log({\"final_score\": mean_oof_score})\n",
    "        wandb_run.finish()\n",
    "\n",
    "    return submission, mean_oof_score, fold_preds_val, models\n",
    "\n",
    "# Вспомогательные функции\n",
    "def get_optimal_xgboost_config():\n",
    "    \"\"\"Конфигурация гиперпараметров\"\"\"\n",
    "    return {\n",
    "        'model_type': 'XGBoost',\n",
    "        'n_estimators': 1000,\n",
    "        'max_depth': 8,\n",
    "        'learning_rate': 0.01,\n",
    "        'subsample': 0.8,\n",
    "        'colsample_bytree': 0.8,\n",
    "        'min_child_weight': 3,\n",
    "        'reg_alpha': 0.01,\n",
    "        'reg_lambda': 1.0,\n",
    "        'gamma': 0,\n",
    "        'seed': SEED,\n",
    "        'early_stopping_patience': 20,\n",
    "        'feature_engineering': 'advanced',\n",
    "    }\n",
    "\n",
    "\n",
    "# Функция создания признаков (пример)\n",
    "def create_additional_xgboost_features(data, is_test=False):\n",
    "    \"\"\"Создание расширенных признаков\"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Базовые признаки\n",
    "    historical_cols = [col for col in data.columns if col.replace('-', '').isdigit()]\n",
    "    if historical_cols:\n",
    "        historical_data = data[historical_cols].values.astype(np.float32)\n",
    "        \n",
    "        # Статистики\n",
    "        features['mean'] = np.mean(historical_data, axis=1)\n",
    "        features['std'] = np.std(historical_data, axis=1)\n",
    "        features['max'] = np.max(historical_data, axis=1)\n",
    "        features['min'] = np.min(historical_data, axis=1)\n",
    "        \n",
    "        # Тренды\n",
    "        x = np.arange(historical_data.shape[1])\n",
    "        slopes = []\n",
    "        for row in historical_data:\n",
    "            coeffs = np.polyfit(x, row, 1)\n",
    "            slopes.append(coeffs[0])\n",
    "        features['trend_slope'] = slopes\n",
    "    \n",
    "    return features"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "databundleVersionId": 11372669,
     "sourceId": 95697,
     "sourceType": "competition"
    }
   ],
   "dockerImageVersionId": 31011,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
